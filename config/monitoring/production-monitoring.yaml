# Production Monitoring Configuration for Rental ML System
# Comprehensive monitoring setup for enterprise-grade data collection and processing

global:
  scrape_interval: 15s
  evaluation_interval: 15s
  external_labels:
    cluster: 'rental-ml-production'
    environment: 'production'

# Alert manager configuration
alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093

# Rule files for alerting
rule_files:
  - "alerts/data-pipeline-alerts.yml"
  - "alerts/kafka-alerts.yml"
  - "alerts/spark-alerts.yml"
  - "alerts/websocket-alerts.yml"
  - "alerts/scraping-alerts.yml"
  - "alerts/quality-alerts.yml"

# Scrape configurations
scrape_configs:
  # Application metrics
  - job_name: 'rental-ml-api'
    static_configs:
      - targets: ['api:8000', 'api-backup:8000']
    metrics_path: '/metrics'
    scrape_interval: 10s
    
  # Kafka metrics
  - job_name: 'kafka'
    static_configs:
      - targets: ['kafka-1:9092', 'kafka-2:9092', 'kafka-3:9092']
    metrics_path: '/metrics'
    scrape_interval: 15s
    
  # Kafka Producer/Consumer metrics
  - job_name: 'kafka-ingestion'
    static_configs:
      - targets: ['ingestion-service:8001']
    metrics_path: '/metrics'
    scrape_interval: 10s
    
  # Apache Pulsar metrics
  - job_name: 'pulsar'
    static_configs:
      - targets: ['pulsar-broker:8080']
    metrics_path: '/metrics'
    scrape_interval: 15s
    
  # Apache Spark metrics
  - job_name: 'spark'
    static_configs:
      - targets: ['spark-master:8080', 'spark-worker-1:8081', 'spark-worker-2:8081']
    metrics_path: '/metrics'
    scrape_interval: 30s
    
  # WebSocket metrics
  - job_name: 'websocket-handler'
    static_configs:
      - targets: ['websocket-service:8002']
    metrics_path: '/metrics'
    scrape_interval: 10s
    
  # Data processing metrics
  - job_name: 'data-processing'
    static_configs:
      - targets: ['processing-service:8003']
    metrics_path: '/metrics'
    scrape_interval: 15s
    
  # Scraping infrastructure metrics
  - job_name: 'scraping-orchestrator'
    static_configs:
      - targets: ['scraping-service:8004']
    metrics_path: '/metrics'
    scrape_interval: 30s
    
  # Data quality metrics
  - job_name: 'data-quality'
    static_configs:
      - targets: ['quality-service:8005']
    metrics_path: '/metrics'
    scrape_interval: 20s
    
  # Schema registry metrics
  - job_name: 'schema-registry'
    static_configs:
      - targets: ['schema-registry:8081']
    metrics_path: '/metrics'
    scrape_interval: 30s
    
  # Database metrics
  - job_name: 'postgresql'
    static_configs:
      - targets: ['postgres-exporter:9187']
    scrape_interval: 15s
    
  # Redis metrics
  - job_name: 'redis'
    static_configs:
      - targets: ['redis-exporter:9121']
    scrape_interval: 15s
    
  # Node exporter for system metrics
  - job_name: 'node-exporter'
    static_configs:
      - targets: ['node-exporter:9100']
    scrape_interval: 15s
    
  # Container metrics
  - job_name: 'cadvisor'
    static_configs:
      - targets: ['cadvisor:8080']
    scrape_interval: 15s
    
  # JVM metrics (for Kafka, Spark)
  - job_name: 'jvm-metrics'
    static_configs:
      - targets: ['jmx-exporter:9404']
    scrape_interval: 30s

# Remote write configuration for long-term storage
remote_write:
  - url: "https://prometheus-remote-write-endpoint/api/v1/write"
    queue_config:
      max_samples_per_send: 1000
      batch_send_deadline: 5s
      max_retries: 3
    
# Recording rules for efficient queries
recording_rules:
  - name: data_pipeline_rates
    rules:
      - record: kafka:message_rate_5m
        expr: rate(kafka_messages_produced_total[5m])
        
      - record: kafka:error_rate_5m
        expr: rate(kafka_messages_produced_total{status="error"}[5m])
        
      - record: websocket:connection_rate_5m
        expr: rate(websocket_active_connections[5m])
        
      - record: data_quality:score_avg_5m
        expr: avg_over_time(data_quality_score[5m])
        
      - record: spark:job_duration_p95_5m
        expr: histogram_quantile(0.95, rate(spark_job_duration_seconds_bucket[5m]))
        
      - record: scraping:success_rate_5m
        expr: rate(scraping_operations_total{status="success"}[5m]) / rate(scraping_operations_total[5m])

# Storage configuration
storage:
  tsdb:
    retention.time: 30d
    retention.size: 100GB
    wal-compression: true