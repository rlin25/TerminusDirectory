"""
Database performance optimization tools for the rental ML system.

This module provides tools for optimizing database performance including
query optimization, indexing strategies, connection pooling, and caching.
"""

import asyncio
import asyncpg
import redis.asyncio as redis
import time
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple, Set
import json
import hashlib
from dataclasses import dataclass, field
from contextlib import asynccontextmanager
import numpy as np

logger = logging.getLogger(__name__)


@dataclass
class QueryAnalysis:
    """Analysis results for a database query."""
    
    query: str
    execution_time_ms: float
    rows_examined: Optional[int] = None
    rows_returned: Optional[int] = None
    index_usage: List[str] = field(default_factory=list)
    suggested_indexes: List[str] = field(default_factory=list)
    optimization_score: float = 0.0
    recommendations: List[str] = field(default_factory=list)
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'query': self.query,
            'execution_time_ms': self.execution_time_ms,
            'rows_examined': self.rows_examined,
            'rows_returned': self.rows_returned,
            'index_usage': self.index_usage,
            'suggested_indexes': self.suggested_indexes,
            'optimization_score': self.optimization_score,
            'recommendations': self.recommendations
        }


@dataclass
class CacheStatistics:
    """Cache performance statistics."""
    
    hits: int = 0
    misses: int = 0
    hit_rate: float = 0.0
    avg_retrieval_time_ms: float = 0.0
    memory_usage_mb: float = 0.0
    evictions: int = 0
    
    def update_hit_rate(self):
        total = self.hits + self.misses
        self.hit_rate = (self.hits / total) if total > 0 else 0.0
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'hits': self.hits,
            'misses': self.misses,
            'hit_rate': self.hit_rate,
            'avg_retrieval_time_ms': self.avg_retrieval_time_ms,
            'memory_usage_mb': self.memory_usage_mb,
            'evictions': self.evictions
        }


class QueryOptimizer:
    """Database query optimization analyzer."""
    
    def __init__(self, db_pool: asyncpg.Pool):
        self.db_pool = db_pool
        self.query_history: List[QueryAnalysis] = []
        self.slow_query_threshold_ms = 1000  # 1 second
    
    async def analyze_query(self, query: str, params: List = None) -> QueryAnalysis:
        """Analyze a query for performance optimization opportunities."""
        
        start_time = time.perf_counter()
        
        async with self.db_pool.acquire() as conn:\n            # Execute EXPLAIN ANALYZE to get query plan\n            explain_query = f\"EXPLAIN (ANALYZE, BUFFERS, FORMAT JSON) {query}\"\n            \n            try:\n                if params:\n                    explain_result = await conn.fetch(explain_query, *params)\n                else:\n                    explain_result = await conn.fetch(explain_query)\n                \n                plan_data = explain_result[0]['QUERY PLAN'][0]\n                \n            except Exception as e:\n                logger.warning(f\"Could not analyze query plan: {e}\")\n                # Fallback to simple execution\n                if params:\n                    await conn.fetch(query, *params)\n                else:\n                    await conn.fetch(query)\n                plan_data = {}\n        \n        execution_time = (time.perf_counter() - start_time) * 1000  # Convert to ms\n        \n        # Extract information from query plan\n        rows_examined = self._extract_rows_examined(plan_data)\n        rows_returned = self._extract_rows_returned(plan_data)\n        index_usage = self._extract_index_usage(plan_data)\n        \n        # Generate optimization recommendations\n        suggested_indexes = self._suggest_indexes(query, plan_data)\n        recommendations = self._generate_recommendations(query, plan_data, execution_time)\n        optimization_score = self._calculate_optimization_score(plan_data, execution_time)\n        \n        analysis = QueryAnalysis(\n            query=query,\n            execution_time_ms=execution_time,\n            rows_examined=rows_examined,\n            rows_returned=rows_returned,\n            index_usage=index_usage,\n            suggested_indexes=suggested_indexes,\n            optimization_score=optimization_score,\n            recommendations=recommendations\n        )\n        \n        self.query_history.append(analysis)\n        \n        if execution_time > self.slow_query_threshold_ms:\n            logger.warning(f\"Slow query detected: {execution_time:.2f}ms - {query[:100]}...\")\n        \n        return analysis\n    \n    def _extract_rows_examined(self, plan_data: Dict) -> Optional[int]:\n        \"\"\"Extract rows examined from query plan.\"\"\"\n        try:\n            return plan_data.get('Plan', {}).get('Actual Rows', 0)\n        except (KeyError, TypeError):\n            return None\n    \n    def _extract_rows_returned(self, plan_data: Dict) -> Optional[int]:\n        \"\"\"Extract rows returned from query plan.\"\"\"\n        try:\n            return plan_data.get('Plan', {}).get('Plan Rows', 0)\n        except (KeyError, TypeError):\n            return None\n    \n    def _extract_index_usage(self, plan_data: Dict) -> List[str]:\n        \"\"\"Extract index usage information from query plan.\"\"\"\n        indexes = []\n        \n        def extract_from_plan(plan):\n            if isinstance(plan, dict):\n                if plan.get('Node Type') == 'Index Scan':\n                    index_name = plan.get('Index Name')\n                    if index_name:\n                        indexes.append(index_name)\n                \n                # Recursively check child plans\n                plans = plan.get('Plans', [])\n                for child_plan in plans:\n                    extract_from_plan(child_plan)\n        \n        try:\n            extract_from_plan(plan_data.get('Plan', {}))\n        except Exception as e:\n            logger.warning(f\"Error extracting index usage: {e}\")\n        \n        return indexes\n    \n    def _suggest_indexes(self, query: str, plan_data: Dict) -> List[str]:\n        \"\"\"Suggest indexes based on query analysis.\"\"\"\n        suggestions = []\n        query_lower = query.lower()\n        \n        # Analyze WHERE clauses\n        if 'where' in query_lower:\n            # Extract column names from WHERE clause (simplified)\n            where_part = query_lower.split('where')[1].split('order by')[0].split('group by')[0]\n            \n            # Look for equality conditions\n            for condition in ['=', 'in (']:\n                if condition in where_part:\n                    # This is a simplified parser - in production, use a proper SQL parser\n                    parts = where_part.split(condition)\n                    if len(parts) >= 2:\n                        column_part = parts[0].strip()\n                        if '.' in column_part:\n                            table, column = column_part.rsplit('.', 1)\n                            table = table.strip()\n                            column = column.strip()\n                            suggestions.append(f\"CREATE INDEX IF NOT EXISTS idx_{table}_{column} ON {table}({column})\")\n        \n        # Analyze JOIN conditions\n        if 'join' in query_lower:\n            # Look for join conditions that might benefit from indexes\n            if 'on' in query_lower:\n                # This is simplified - in production, use proper SQL parsing\n                suggestions.append(\"Consider indexes on JOIN columns\")\n        \n        # Analyze ORDER BY clauses\n        if 'order by' in query_lower:\n            order_part = query_lower.split('order by')[1].split('limit')[0] if 'limit' in query_lower else query_lower.split('order by')[1]\n            order_part = order_part.strip()\n            \n            if '.' in order_part:\n                suggestions.append(f\"Consider index for ORDER BY: {order_part}\")\n        \n        return suggestions\n    \n    def _generate_recommendations(self, query: str, plan_data: Dict, execution_time: float) -> List[str]:\n        \"\"\"Generate optimization recommendations.\"\"\"\n        recommendations = []\n        \n        # Slow query recommendations\n        if execution_time > self.slow_query_threshold_ms:\n            recommendations.append(f\"Query is slow ({execution_time:.2f}ms). Consider optimization.\")\n        \n        # Sequential scan recommendations\n        if self._has_sequential_scan(plan_data):\n            recommendations.append(\"Sequential scan detected. Consider adding appropriate indexes.\")\n        \n        # High row examination ratio\n        rows_examined = self._extract_rows_examined(plan_data)\n        rows_returned = self._extract_rows_returned(plan_data)\n        \n        if rows_examined and rows_returned and rows_examined > rows_returned * 10:\n            recommendations.append(f\"High examination ratio ({rows_examined}:{rows_returned}). Consider more selective indexes.\")\n        \n        # Large result set without LIMIT\n        if rows_returned and rows_returned > 1000 and 'limit' not in query.lower():\n            recommendations.append(\"Large result set returned. Consider adding LIMIT clause or pagination.\")\n        \n        # Complex aggregations\n        if any(keyword in query.lower() for keyword in ['group by', 'having', 'count(', 'sum(', 'avg(']):\n            recommendations.append(\"Complex aggregation query. Consider materialized views for frequently accessed aggregations.\")\n        \n        return recommendations\n    \n    def _has_sequential_scan(self, plan_data: Dict) -> bool:\n        \"\"\"Check if query plan contains sequential scans.\"\"\"\n        def check_plan(plan):\n            if isinstance(plan, dict):\n                if plan.get('Node Type') == 'Seq Scan':\n                    return True\n                \n                plans = plan.get('Plans', [])\n                for child_plan in plans:\n                    if check_plan(child_plan):\n                        return True\n            return False\n        \n        try:\n            return check_plan(plan_data.get('Plan', {}))\n        except Exception:\n            return False\n    \n    def _calculate_optimization_score(self, plan_data: Dict, execution_time: float) -> float:\n        \"\"\"Calculate optimization score (0-100, higher is better).\"\"\"\n        score = 100.0\n        \n        # Penalize slow queries\n        if execution_time > self.slow_query_threshold_ms:\n            score -= min(50, (execution_time - self.slow_query_threshold_ms) / 100)\n        \n        # Penalize sequential scans\n        if self._has_sequential_scan(plan_data):\n            score -= 20\n        \n        # Penalize high examination ratios\n        rows_examined = self._extract_rows_examined(plan_data)\n        rows_returned = self._extract_rows_returned(plan_data)\n        \n        if rows_examined and rows_returned and rows_examined > 0:\n            ratio = rows_examined / max(rows_returned, 1)\n            if ratio > 10:\n                score -= min(20, ratio / 10)\n        \n        return max(0, score)\n    \n    def get_slow_queries(self, threshold_ms: float = None) -> List[QueryAnalysis]:\n        \"\"\"Get list of slow queries.\"\"\"\n        threshold = threshold_ms or self.slow_query_threshold_ms\n        return [q for q in self.query_history if q.execution_time_ms > threshold]\n    \n    def get_optimization_summary(self) -> Dict[str, Any]:\n        \"\"\"Get summary of optimization opportunities.\"\"\"\n        if not self.query_history:\n            return {'error': 'No query history available'}\n        \n        slow_queries = self.get_slow_queries()\n        avg_execution_time = np.mean([q.execution_time_ms for q in self.query_history])\n        avg_optimization_score = np.mean([q.optimization_score for q in self.query_history])\n        \n        # Collect all recommendations\n        all_recommendations = {}\n        for query_analysis in self.query_history:\n            for rec in query_analysis.recommendations:\n                all_recommendations[rec] = all_recommendations.get(rec, 0) + 1\n        \n        # Sort recommendations by frequency\n        top_recommendations = sorted(\n            all_recommendations.items(), \n            key=lambda x: x[1], \n            reverse=True\n        )[:10]\n        \n        return {\n            'total_queries_analyzed': len(self.query_history),\n            'slow_queries_count': len(slow_queries),\n            'avg_execution_time_ms': avg_execution_time,\n            'avg_optimization_score': avg_optimization_score,\n            'top_recommendations': [{'recommendation': rec, 'frequency': freq} for rec, freq in top_recommendations],\n            'optimization_opportunities': len([q for q in self.query_history if q.optimization_score < 80])\n        }\n\n\nclass CacheOptimizer:\n    \"\"\"Redis cache optimization manager.\"\"\"\n    \n    def __init__(self, redis_client: redis.Redis):\n        self.redis_client = redis_client\n        self.stats = CacheStatistics()\n        self.cache_patterns: Dict[str, int] = {}  # Track cache key patterns\n        self.optimal_ttl_analysis: Dict[str, List[float]] = {}  # Track TTL effectiveness\n    \n    async def get_with_stats(self, key: str) -> Optional[str]:\n        \"\"\"Get value from cache with statistics tracking.\"\"\"\n        start_time = time.perf_counter()\n        \n        try:\n            value = await self.redis_client.get(key)\n            \n            retrieval_time = (time.perf_counter() - start_time) * 1000  # ms\n            \n            if value is not None:\n                self.stats.hits += 1\n                self._track_cache_pattern(key, hit=True)\n            else:\n                self.stats.misses += 1\n                self._track_cache_pattern(key, hit=False)\n            \n            self.stats.update_hit_rate()\n            \n            # Update average retrieval time\n            total_requests = self.stats.hits + self.stats.misses\n            self.stats.avg_retrieval_time_ms = (\n                (self.stats.avg_retrieval_time_ms * (total_requests - 1) + retrieval_time) / total_requests\n            )\n            \n            return value\n            \n        except Exception as e:\n            logger.error(f\"Cache get error for key {key}: {e}\")\n            self.stats.misses += 1\n            return None\n    \n    async def set_with_optimization(self, key: str, value: str, ttl: int = 3600) -> bool:\n        \"\"\"Set value in cache with optimization tracking.\"\"\"\n        try:\n            # Optimize TTL based on historical data\n            optimized_ttl = self._optimize_ttl(key, ttl)\n            \n            result = await self.redis_client.set(key, value, ex=optimized_ttl)\n            \n            # Track TTL usage for future optimization\n            if key not in self.optimal_ttl_analysis:\n                self.optimal_ttl_analysis[key] = []\n            self.optimal_ttl_analysis[key].append(optimized_ttl)\n            \n            return result\n            \n        except Exception as e:\n            logger.error(f\"Cache set error for key {key}: {e}\")\n            return False\n    \n    def _track_cache_pattern(self, key: str, hit: bool):\n        \"\"\"Track cache key patterns for optimization.\"\"\"\n        # Extract pattern from key (e.g., \"user:123:preferences\" -> \"user:*:preferences\")\n        pattern = self._extract_pattern(key)\n        \n        if pattern not in self.cache_patterns:\n            self.cache_patterns[pattern] = 0\n        \n        if hit:\n            self.cache_patterns[pattern] += 1\n    \n    def _extract_pattern(self, key: str) -> str:\n        \"\"\"Extract pattern from cache key.\"\"\"\n        # Simple pattern extraction - replace numeric IDs with wildcard\n        import re\n        pattern = re.sub(r'\\d+', '*', key)\n        return pattern\n    \n    def _optimize_ttl(self, key: str, default_ttl: int) -> int:\n        \"\"\"Optimize TTL based on historical access patterns.\"\"\"\n        pattern = self._extract_pattern(key)\n        \n        # If this pattern is frequently accessed, use longer TTL\n        if pattern in self.cache_patterns:\n            access_frequency = self.cache_patterns[pattern]\n            \n            if access_frequency > 100:  # High frequency\n                return min(default_ttl * 2, 7200)  # Max 2 hours\n            elif access_frequency > 50:  # Medium frequency\n                return int(default_ttl * 1.5)\n            elif access_frequency < 10:  # Low frequency\n                return max(default_ttl // 2, 300)  # Min 5 minutes\n        \n        return default_ttl\n    \n    async def analyze_memory_usage(self) -> Dict[str, Any]:\n        \"\"\"Analyze Redis memory usage and optimization opportunities.\"\"\"\n        try:\n            info = await self.redis_client.info('memory')\n            \n            memory_usage_mb = info.get('used_memory', 0) / 1024 / 1024\n            max_memory_mb = info.get('maxmemory', 0) / 1024 / 1024\n            \n            self.stats.memory_usage_mb = memory_usage_mb\n            \n            # Get key statistics\n            keyspace_info = await self.redis_client.info('keyspace')\n            total_keys = sum(\n                int(db_info.split('keys=')[1].split(',')[0]) \n                for db_info in keyspace_info.values() \n                if isinstance(db_info, str) and 'keys=' in db_info\n            )\n            \n            # Analyze key patterns\n            pattern_analysis = {}\n            for pattern, hits in self.cache_patterns.items():\n                pattern_analysis[pattern] = {\n                    'hits': hits,\n                    'hit_rate': hits / max(self.stats.hits, 1),\n                    'recommendation': self._get_pattern_recommendation(pattern, hits)\n                }\n            \n            return {\n                'memory_usage_mb': memory_usage_mb,\n                'max_memory_mb': max_memory_mb,\n                'memory_utilization': (memory_usage_mb / max_memory_mb) if max_memory_mb > 0 else 0,\n                'total_keys': total_keys,\n                'avg_key_size_bytes': (memory_usage_mb * 1024 * 1024) / max(total_keys, 1),\n                'pattern_analysis': pattern_analysis,\n                'recommendations': self._get_memory_recommendations(memory_usage_mb, max_memory_mb, total_keys)\n            }\n            \n        except Exception as e:\n            logger.error(f\"Error analyzing memory usage: {e}\")\n            return {'error': str(e)}\n    \n    def _get_pattern_recommendation(self, pattern: str, hits: int) -> str:\n        \"\"\"Get recommendation for cache pattern.\"\"\"\n        if hits > 1000:\n            return \"High-value pattern. Consider longer TTL or pre-warming.\"\n        elif hits < 10:\n            return \"Low-value pattern. Consider shorter TTL or removal.\"\n        else:\n            return \"Normal pattern. Current settings appear appropriate.\"\n    \n    def _get_memory_recommendations(self, used_mb: float, max_mb: float, total_keys: int) -> List[str]:\n        \"\"\"Get memory optimization recommendations.\"\"\"\n        recommendations = []\n        \n        if max_mb > 0:\n            utilization = used_mb / max_mb\n            \n            if utilization > 0.8:\n                recommendations.append(\"High memory utilization. Consider increasing memory or implementing LRU eviction.\")\n            elif utilization > 0.6:\n                recommendations.append(\"Moderate memory utilization. Monitor growth trends.\")\n        \n        avg_key_size = (used_mb * 1024 * 1024) / max(total_keys, 1)\n        if avg_key_size > 10240:  # 10KB\n            recommendations.append(\"Large average key size. Consider compression or data structure optimization.\")\n        \n        if self.stats.hit_rate < 0.7:\n            recommendations.append(f\"Low cache hit rate ({self.stats.hit_rate:.1%}). Review caching strategy.\")\n        \n        return recommendations\n    \n    def get_cache_summary(self) -> Dict[str, Any]:\n        \"\"\"Get comprehensive cache performance summary.\"\"\"\n        return {\n            'statistics': self.stats.to_dict(),\n            'top_patterns': sorted(\n                [(pattern, hits) for pattern, hits in self.cache_patterns.items()],\n                key=lambda x: x[1],\n                reverse=True\n            )[:10],\n            'ttl_analysis': {\n                pattern: {\n                    'avg_ttl': np.mean(ttls),\n                    'min_ttl': min(ttls),\n                    'max_ttl': max(ttls),\n                    'count': len(ttls)\n                }\n                for pattern, ttls in self.optimal_ttl_analysis.items()\n                if ttls\n            }\n        }\n\n\nclass ConnectionPoolOptimizer:\n    \"\"\"Database connection pool optimization manager.\"\"\"\n    \n    def __init__(self, db_pool: asyncpg.Pool):\n        self.db_pool = db_pool\n        self.connection_stats = {\n            'acquisitions': 0,\n            'releases': 0,\n            'timeouts': 0,\n            'total_wait_time': 0.0,\n            'active_connections': 0,\n            'peak_connections': 0\n        }\n    \n    @asynccontextmanager\n    async def acquire_with_stats(self):\n        \"\"\"Acquire connection with performance tracking.\"\"\"\n        start_time = time.perf_counter()\n        \n        try:\n            async with self.db_pool.acquire() as conn:\n                wait_time = time.perf_counter() - start_time\n                \n                self.connection_stats['acquisitions'] += 1\n                self.connection_stats['total_wait_time'] += wait_time\n                self.connection_stats['active_connections'] += 1\n                self.connection_stats['peak_connections'] = max(\n                    self.connection_stats['peak_connections'],\n                    self.connection_stats['active_connections']\n                )\n                \n                yield conn\n                \n        except asyncio.TimeoutError:\n            self.connection_stats['timeouts'] += 1\n            raise\n        except Exception:\n            raise\n        finally:\n            self.connection_stats['releases'] += 1\n            self.connection_stats['active_connections'] = max(0, self.connection_stats['active_connections'] - 1)\n    \n    def get_pool_statistics(self) -> Dict[str, Any]:\n        \"\"\"Get connection pool performance statistics.\"\"\"\n        total_acquisitions = self.connection_stats['acquisitions']\n        avg_wait_time = (\n            self.connection_stats['total_wait_time'] / max(total_acquisitions, 1)\n        )\n        \n        timeout_rate = (\n            self.connection_stats['timeouts'] / max(total_acquisitions, 1)\n        )\n        \n        recommendations = []\n        \n        # Analyze performance and generate recommendations\n        if avg_wait_time > 0.1:  # 100ms\n            recommendations.append(f\"High average wait time ({avg_wait_time:.3f}s). Consider increasing pool size.\")\n        \n        if timeout_rate > 0.01:  # 1%\n            recommendations.append(f\"High timeout rate ({timeout_rate:.1%}). Consider increasing pool size or timeout.\")\n        \n        if self.connection_stats['peak_connections'] > self.db_pool.get_size() * 0.9:\n            recommendations.append(\"Peak usage near pool limit. Consider increasing pool size.\")\n        \n        return {\n            'pool_size': self.db_pool.get_size(),\n            'current_active': self.connection_stats['active_connections'],\n            'peak_connections': self.connection_stats['peak_connections'],\n            'total_acquisitions': total_acquisitions,\n            'total_releases': self.connection_stats['releases'],\n            'timeouts': self.connection_stats['timeouts'],\n            'avg_wait_time_ms': avg_wait_time * 1000,\n            'timeout_rate': timeout_rate,\n            'recommendations': recommendations\n        }\n\n\nclass DatabaseOptimizer:\n    \"\"\"Comprehensive database optimization manager.\"\"\"\n    \n    def __init__(self, db_pool: asyncpg.Pool, redis_client: redis.Redis):\n        self.db_pool = db_pool\n        self.redis_client = redis_client\n        \n        self.query_optimizer = QueryOptimizer(db_pool)\n        self.cache_optimizer = CacheOptimizer(redis_client)\n        self.pool_optimizer = ConnectionPoolOptimizer(db_pool)\n        \n        self.optimization_history: List[Dict[str, Any]] = []\n    \n    async def optimize_query(self, query: str, params: List = None) -> QueryAnalysis:\n        \"\"\"Optimize a single query.\"\"\"\n        return await self.query_optimizer.analyze_query(query, params)\n    \n    async def cached_query(self, query: str, params: List = None, ttl: int = 3600) -> Any:\n        \"\"\"Execute query with intelligent caching.\"\"\"\n        # Create cache key\n        cache_key = self._create_cache_key(query, params)\n        \n        # Try to get from cache\n        cached_result = await self.cache_optimizer.get_with_stats(cache_key)\n        \n        if cached_result is not None:\n            return json.loads(cached_result)\n        \n        # Execute query with optimization analysis\n        analysis = await self.query_optimizer.analyze_query(query, params)\n        \n        # Execute actual query\n        async with self.pool_optimizer.acquire_with_stats() as conn:\n            if params:\n                result = await conn.fetch(query, *params)\n            else:\n                result = await conn.fetch(query)\n        \n        # Convert result to serializable format\n        serializable_result = [dict(row) for row in result]\n        \n        # Cache the result\n        await self.cache_optimizer.set_with_optimization(\n            cache_key, \n            json.dumps(serializable_result, default=str),\n            ttl\n        )\n        \n        return serializable_result\n    \n    def _create_cache_key(self, query: str, params: List = None) -> str:\n        \"\"\"Create cache key for query and parameters.\"\"\"\n        # Normalize query\n        normalized_query = ' '.join(query.split())\n        \n        # Create hash of query and parameters\n        key_data = f\"{normalized_query}:{params or []}\"\n        cache_key = hashlib.md5(key_data.encode()).hexdigest()\n        \n        return f\"query:{cache_key}\"\n    \n    async def run_optimization_analysis(self) -> Dict[str, Any]:\n        \"\"\"Run comprehensive optimization analysis.\"\"\"\n        logger.info(\"Running comprehensive database optimization analysis...\")\n        \n        analysis = {\n            'timestamp': datetime.now().isoformat(),\n            'query_optimization': self.query_optimizer.get_optimization_summary(),\n            'cache_performance': self.cache_optimizer.get_cache_summary(),\n            'connection_pool': self.pool_optimizer.get_pool_statistics(),\n            'memory_analysis': await self.cache_optimizer.analyze_memory_usage(),\n            'overall_recommendations': []\n        }\n        \n        # Generate overall recommendations\n        overall_recommendations = []\n        \n        # Query optimization recommendations\n        query_summary = analysis['query_optimization']\n        if query_summary.get('avg_optimization_score', 100) < 70:\n            overall_recommendations.append(\n                \"Low average query optimization score. Review slow queries and implement suggested indexes.\"\n            )\n        \n        # Cache performance recommendations\n        cache_stats = analysis['cache_performance']['statistics']\n        if cache_stats['hit_rate'] < 0.7:\n            overall_recommendations.append(\n                f\"Low cache hit rate ({cache_stats['hit_rate']:.1%}). Review caching strategy and TTL settings.\"\n            )\n        \n        # Connection pool recommendations\n        pool_stats = analysis['connection_pool']\n        if pool_stats['timeout_rate'] > 0.01:\n            overall_recommendations.append(\n                \"High connection pool timeout rate. Consider increasing pool size or optimizing query performance.\"\n            )\n        \n        analysis['overall_recommendations'] = overall_recommendations\n        \n        # Store analysis in history\n        self.optimization_history.append(analysis)\n        \n        logger.info(\"Database optimization analysis completed\")\n        return analysis\n    \n    async def apply_automatic_optimizations(self) -> Dict[str, Any]:\n        \"\"\"Apply automatic optimizations where safe to do so.\"\"\"\n        logger.info(\"Applying automatic database optimizations...\")\n        \n        applied_optimizations = []\n        \n        try:\n            # Optimize Redis memory if needed\n            memory_analysis = await self.cache_optimizer.analyze_memory_usage()\n            \n            if memory_analysis.get('memory_utilization', 0) > 0.8:\n                # Clean up low-value cache patterns\n                cleaned_keys = await self._cleanup_low_value_cache_keys()\n                applied_optimizations.append(f\"Cleaned up {cleaned_keys} low-value cache keys\")\n            \n            # Optimize connection pool settings if needed\n            pool_stats = self.pool_optimizer.get_pool_statistics()\n            \n            if pool_stats['timeout_rate'] > 0.05:  # 5% timeout rate\n                # This would require recreating the pool, which is complex\n                applied_optimizations.append(\"Recommended: Increase connection pool size\")\n            \n            # Generate index creation scripts for common patterns\n            index_scripts = self._generate_index_scripts()\n            if index_scripts:\n                applied_optimizations.append(f\"Generated {len(index_scripts)} index creation scripts\")\n            \n            return {\n                'timestamp': datetime.now().isoformat(),\n                'applied_optimizations': applied_optimizations,\n                'index_scripts': index_scripts,\n                'status': 'success'\n            }\n            \n        except Exception as e:\n            logger.error(f\"Error applying automatic optimizations: {e}\")\n            return {\n                'timestamp': datetime.now().isoformat(),\n                'error': str(e),\n                'status': 'failed'\n            }\n    \n    async def _cleanup_low_value_cache_keys(self) -> int:\n        \"\"\"Clean up cache keys with low hit rates.\"\"\"\n        cleaned_count = 0\n        \n        for pattern, hits in self.cache_optimizer.cache_patterns.items():\n            # Remove patterns with very low hit counts\n            if hits < 5 and self.cache_optimizer.stats.hits > 100:\n                # In a real implementation, you'd need to identify and delete actual keys\n                # This is a simplified example\n                logger.info(f\"Would clean up low-value pattern: {pattern}\")\n                cleaned_count += 1\n        \n        return cleaned_count\n    \n    def _generate_index_scripts(self) -> List[str]:\n        \"\"\"Generate index creation scripts based on query analysis.\"\"\"\n        scripts = []\n        \n        for analysis in self.query_optimizer.query_history:\n            for suggested_index in analysis.suggested_indexes:\n                if suggested_index not in scripts:\n                    scripts.append(suggested_index)\n        \n        return scripts\n    \n    def export_optimization_report(self, filename: str):\n        \"\"\"Export comprehensive optimization report.\"\"\"\n        report = {\n            'export_timestamp': datetime.now().isoformat(),\n            'optimization_history': self.optimization_history,\n            'current_statistics': {\n                'query_optimizer': self.query_optimizer.get_optimization_summary(),\n                'cache_optimizer': self.cache_optimizer.get_cache_summary(),\n                'pool_optimizer': self.pool_optimizer.get_pool_statistics()\n            },\n            'recommendations': self._generate_comprehensive_recommendations()\n        }\n        \n        with open(filename, 'w') as f:\n            json.dump(report, f, indent=2, default=str)\n        \n        logger.info(f\"Optimization report exported to {filename}\")\n    \n    def _generate_comprehensive_recommendations(self) -> Dict[str, List[str]]:\n        \"\"\"Generate comprehensive optimization recommendations.\"\"\"\n        recommendations = {\n            'immediate_actions': [],\n            'short_term_improvements': [],\n            'long_term_strategy': []\n        }\n        \n        # Immediate actions based on current performance\n        slow_queries = self.query_optimizer.get_slow_queries()\n        if len(slow_queries) > 10:\n            recommendations['immediate_actions'].append(\n                f\"Address {len(slow_queries)} slow queries immediately\"\n            )\n        \n        if self.cache_optimizer.stats.hit_rate < 0.5:\n            recommendations['immediate_actions'].append(\n                \"Critically low cache hit rate - review caching strategy\"\n            )\n        \n        # Short-term improvements\n        if self.query_optimizer.get_optimization_summary().get('avg_optimization_score', 100) < 80:\n            recommendations['short_term_improvements'].append(\n                \"Implement suggested database indexes\"\n            )\n        \n        recommendations['short_term_improvements'].append(\n            \"Set up automated performance monitoring and alerting\"\n        )\n        \n        # Long-term strategy\n        recommendations['long_term_strategy'].extend([\n            \"Consider read replicas for read-heavy workloads\",\n            \"Evaluate database partitioning for large tables\",\n            \"Implement automated query optimization\",\n            \"Consider database-specific optimization features\"\n        ])\n        \n        return recommendations\n\n\nif __name__ == \"__main__\":\n    import asyncio\n    \n    async def example_usage():\n        \"\"\"Example usage of the database optimizer.\"\"\"\n        # This is a demonstration - in real usage, you'd use actual connections\n        \n        # Mock connections for demonstration\n        class MockPool:\n            def get_size(self):\n                return 10\n        \n        class MockRedis:\n            async def get(self, key):\n                return None\n            \n            async def set(self, key, value, ex=None):\n                return True\n            \n            async def info(self, section=None):\n                return {'used_memory': 1024 * 1024 * 100}  # 100MB\n        \n        mock_pool = MockPool()\n        mock_redis = MockRedis()\n        \n        # This would fail with mock objects, but shows the interface\n        try:\n            optimizer = DatabaseOptimizer(mock_pool, mock_redis)\n            \n            # Run analysis\n            analysis = await optimizer.run_optimization_analysis()\n            print(\"Optimization analysis:\", json.dumps(analysis, indent=2, default=str))\n            \n        except Exception as e:\n            print(f\"Example failed (expected with mock objects): {e}\")\n            \n            # Show the structure that would be returned\n            example_analysis = {\n                'timestamp': datetime.now().isoformat(),\n                'query_optimization': {\n                    'total_queries_analyzed': 0,\n                    'slow_queries_count': 0,\n                    'avg_optimization_score': 85.0\n                },\n                'cache_performance': {\n                    'statistics': {\n                        'hit_rate': 0.75,\n                        'avg_retrieval_time_ms': 2.5\n                    }\n                },\n                'overall_recommendations': [\n                    \"Monitor query performance regularly\",\n                    \"Implement caching for frequently accessed data\"\n                ]\n            }\n            \n            print(\"Example analysis structure:\")\n            print(json.dumps(example_analysis, indent=2))\n    \n    # Run example\n    asyncio.run(example_usage())"