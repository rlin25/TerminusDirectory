"""
ML model performance optimization tools for the rental ML system.

This module provides tools for optimizing ML model inference performance,
including model quantization, pruning, batching, and caching strategies.
"""

import numpy as np
import tensorflow as tf
import time
import logging
import pickle
import json
import hashlib
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple, Union, Callable
from dataclasses import dataclass, field
from pathlib import Path
import threading
from concurrent.futures import ThreadPoolExecutor
import asyncio
import gc

logger = logging.getLogger(__name__)


@dataclass
class ModelPerformanceMetrics:
    \"\"\"Container for ML model performance metrics.\"\"\"\n    \n    model_name: str\n    version: str\n    timestamp: datetime\n    \n    # Inference performance\n    avg_inference_time_ms: float\n    p95_inference_time_ms: float\n    throughput_predictions_per_sec: float\n    \n    # Memory usage\n    model_memory_mb: float\n    inference_memory_mb: float\n    peak_memory_mb: float\n    \n    # Accuracy metrics\n    accuracy_score: Optional[float] = None\n    precision_score: Optional[float] = None\n    recall_score: Optional[float] = None\n    f1_score: Optional[float] = None\n    \n    # Optimization details\n    batch_size: int = 1\n    quantization_applied: bool = False\n    pruning_applied: bool = False\n    optimization_techniques: List[str] = field(default_factory=list)\n    \n    # Resource efficiency\n    predictions_per_mb: float = 0.0\n    efficiency_score: float = 0.0\n    \n    def to_dict(self) -> Dict[str, Any]:\n        return {\n            'model_name': self.model_name,\n            'version': self.version,\n            'timestamp': self.timestamp.isoformat(),\n            'avg_inference_time_ms': self.avg_inference_time_ms,\n            'p95_inference_time_ms': self.p95_inference_time_ms,\n            'throughput_predictions_per_sec': self.throughput_predictions_per_sec,\n            'model_memory_mb': self.model_memory_mb,\n            'inference_memory_mb': self.inference_memory_mb,\n            'peak_memory_mb': self.peak_memory_mb,\n            'accuracy_score': self.accuracy_score,\n            'precision_score': self.precision_score,\n            'recall_score': self.recall_score,\n            'f1_score': self.f1_score,\n            'batch_size': self.batch_size,\n            'quantization_applied': self.quantization_applied,\n            'pruning_applied': self.pruning_applied,\n            'optimization_techniques': self.optimization_techniques,\n            'predictions_per_mb': self.predictions_per_mb,\n            'efficiency_score': self.efficiency_score\n        }


class ModelOptimizer:\n    \"\"\"Base class for ML model optimization.\"\"\"\n    \n    def __init__(self, model_name: str):\n        self.model_name = model_name\n        self.optimization_history: List[ModelPerformanceMetrics] = []\n        self.baseline_metrics: Optional[ModelPerformanceMetrics] = None\n    \n    def set_baseline(self, metrics: ModelPerformanceMetrics):\n        \"\"\"Set baseline performance metrics.\"\"\"\n        self.baseline_metrics = metrics\n        logger.info(f\"Set baseline for {self.model_name}: {metrics.avg_inference_time_ms:.2f}ms\")\n    \n    def compare_with_baseline(self, current_metrics: ModelPerformanceMetrics) -> Dict[str, Any]:\n        \"\"\"Compare current metrics with baseline.\"\"\"\n        if not self.baseline_metrics:\n            return {'error': 'No baseline metrics set'}\n        \n        baseline = self.baseline_metrics\n        current = current_metrics\n        \n        comparison = {\n            'model_name': self.model_name,\n            'baseline_version': baseline.version,\n            'current_version': current.version,\n            'improvements': {},\n            'regressions': {},\n            'overall_improvement': False\n        }\n        \n        # Performance comparisons\n        metrics_to_compare = [\n            ('avg_inference_time_ms', 'lower_is_better'),\n            ('model_memory_mb', 'lower_is_better'),\n            ('throughput_predictions_per_sec', 'higher_is_better'),\n            ('accuracy_score', 'higher_is_better'),\n            ('efficiency_score', 'higher_is_better')\n        ]\n        \n        for metric_name, direction in metrics_to_compare:\n            baseline_value = getattr(baseline, metric_name)\n            current_value = getattr(current, metric_name)\n            \n            if baseline_value is None or current_value is None:\n                continue\n            \n            if baseline_value == 0:\n                change_percent = float('inf') if current_value != 0 else 0.0\n            else:\n                change_percent = ((current_value - baseline_value) / baseline_value) * 100\n            \n            is_improvement = (\n                (direction == 'lower_is_better' and change_percent < 0) or\n                (direction == 'higher_is_better' and change_percent > 0)\n            )\n            \n            comparison_data = {\n                'baseline': baseline_value,\n                'current': current_value,\n                'change_percent': change_percent,\n                'is_improvement': is_improvement\n            }\n            \n            if is_improvement:\n                comparison['improvements'][metric_name] = comparison_data\n            elif abs(change_percent) > 5:  # Significant change threshold\n                comparison['regressions'][metric_name] = comparison_data\n        \n        # Determine overall improvement\n        comparison['overall_improvement'] = (\n            len(comparison['improvements']) > len(comparison['regressions'])\n        )\n        \n        return comparison\n\n\nclass TensorFlowOptimizer(ModelOptimizer):\n    \"\"\"TensorFlow model optimization utilities.\"\"\"\n    \n    def __init__(self, model: tf.keras.Model, model_name: str):\n        super().__init__(model_name)\n        self.original_model = model\n        self.optimized_models: Dict[str, tf.keras.Model] = {}\n        self.quantization_cache: Dict[str, Any] = {}\n    \n    def apply_quantization(self, model: tf.keras.Model, \n                          quantization_type: str = \"dynamic\") -> tf.keras.Model:\n        \"\"\"Apply model quantization for reduced memory and faster inference.\"\"\"\n        logger.info(f\"Applying {quantization_type} quantization to {self.model_name}\")\n        \n        try:\n            if quantization_type == \"dynamic\":\n                # Dynamic range quantization\n                converter = tf.lite.TFLiteConverter.from_keras_model(model)\n                converter.optimizations = [tf.lite.Optimize.DEFAULT]\n                quantized_tflite = converter.convert()\n                \n                # Save and reload as TensorFlow model\n                tflite_path = f\"/tmp/{self.model_name}_quantized.tflite\"\n                with open(tflite_path, 'wb') as f:\n                    f.write(quantized_tflite)\n                \n                # Create interpreter for the quantized model\n                interpreter = tf.lite.Interpreter(model_path=tflite_path)\n                interpreter.allocate_tensors()\n                \n                # Store interpreter in cache\n                self.quantization_cache[f\"{quantization_type}_interpreter\"] = interpreter\n                \n                logger.info(f\"Dynamic quantization applied successfully\")\n                return model  # Return original for now, use interpreter for inference\n                \n            elif quantization_type == \"int8\":\n                # Integer quantization (requires representative dataset)\n                converter = tf.lite.TFLiteConverter.from_keras_model(model)\n                converter.optimizations = [tf.lite.Optimize.DEFAULT]\n                converter.target_spec.supported_types = [tf.int8]\n                \n                # Note: In production, you'd provide a representative dataset\n                # converter.representative_dataset = representative_dataset_gen\n                \n                quantized_tflite = converter.convert()\n                \n                tflite_path = f\"/tmp/{self.model_name}_int8_quantized.tflite\"\n                with open(tflite_path, 'wb') as f:\n                    f.write(quantized_tflite)\n                \n                interpreter = tf.lite.Interpreter(model_path=tflite_path)\n                interpreter.allocate_tensors()\n                \n                self.quantization_cache[f\"{quantization_type}_interpreter\"] = interpreter\n                \n                logger.info(f\"INT8 quantization applied successfully\")\n                return model\n                \n            else:\n                raise ValueError(f\"Unsupported quantization type: {quantization_type}\")\n                \n        except Exception as e:\n            logger.error(f\"Quantization failed: {e}\")\n            return model\n    \n    def apply_pruning(self, model: tf.keras.Model, sparsity: float = 0.5) -> tf.keras.Model:\n        \"\"\"Apply model pruning to reduce model size.\"\"\"\n        logger.info(f\"Applying pruning with {sparsity:.1%} sparsity to {self.model_name}\")\n        \n        try:\n            import tensorflow_model_optimization as tfmot\n            \n            # Define pruning schedule\n            pruning_schedule = tfmot.sparsity.keras.ConstantSparsity(\n                target_sparsity=sparsity,\n                begin_step=0\n            )\n            \n            # Apply pruning to the model\n            pruned_model = tfmot.sparsity.keras.prune_low_magnitude(\n                model,\n                pruning_schedule=pruning_schedule\n            )\n            \n            # Compile the pruned model\n            pruned_model.compile(\n                optimizer=model.optimizer,\n                loss=model.loss,\n                metrics=model.metrics\n            )\n            \n            logger.info(f\"Pruning applied successfully with {sparsity:.1%} sparsity\")\n            return pruned_model\n            \n        except ImportError:\n            logger.warning(\"TensorFlow Model Optimization not available. Install with: pip install tensorflow-model-optimization\")\n            return model\n        except Exception as e:\n            logger.error(f\"Pruning failed: {e}\")\n            return model\n    \n    def optimize_for_inference(self, model: tf.keras.Model) -> tf.keras.Model:\n        \"\"\"Apply general inference optimizations.\"\"\"\n        logger.info(f\"Optimizing {self.model_name} for inference\")\n        \n        try:\n            # Convert to TensorFlow Lite for optimization\n            converter = tf.lite.TFLiteConverter.from_keras_model(model)\n            \n            # Apply optimizations\n            converter.optimizations = [tf.lite.Optimize.DEFAULT]\n            \n            # Enable experimental optimizations\n            converter.experimental_new_converter = True\n            converter.experimental_new_quantizer = True\n            \n            # Convert and save\n            optimized_tflite = converter.convert()\n            \n            tflite_path = f\"/tmp/{self.model_name}_optimized.tflite\"\n            with open(tflite_path, 'wb') as f:\n                f.write(optimized_tflite)\n            \n            # Create interpreter\n            interpreter = tf.lite.Interpreter(model_path=tflite_path)\n            interpreter.allocate_tensors()\n            \n            self.quantization_cache[\"optimized_interpreter\"] = interpreter\n            \n            logger.info(\"Model optimized for inference\")\n            return model\n            \n        except Exception as e:\n            logger.error(f\"Inference optimization failed: {e}\")\n            return model\n    \n    def benchmark_model(self, model: tf.keras.Model, \n                       test_data: np.ndarray, \n                       batch_sizes: List[int] = [1, 8, 16, 32, 64],\n                       iterations: int = 100) -> Dict[str, ModelPerformanceMetrics]:\n        \"\"\"Benchmark model performance across different batch sizes.\"\"\"\n        logger.info(f\"Benchmarking {self.model_name} with batch sizes: {batch_sizes}\")\n        \n        results = {}\n        \n        for batch_size in batch_sizes:\n            logger.info(f\"Testing batch size: {batch_size}\")\n            \n            # Prepare test batches\n            num_samples = min(len(test_data), batch_size * 10)  # Limit test data\n            batched_data = test_data[:num_samples].reshape(-1, batch_size, *test_data.shape[1:])\n            \n            inference_times = []\n            memory_usage = []\n            \n            # Warm up\n            for _ in range(5):\n                if len(batched_data) > 0:\n                    _ = model.predict(batched_data[0], verbose=0)\n            \n            # Benchmark iterations\n            for i in range(min(iterations, len(batched_data))):\n                # Monitor memory before inference\n                import psutil\n                process = psutil.Process()\n                memory_before = process.memory_info().rss / 1024 / 1024  # MB\n                \n                # Time inference\n                start_time = time.perf_counter()\n                predictions = model.predict(batched_data[i], verbose=0)\n                end_time = time.perf_counter()\n                \n                # Monitor memory after inference\n                memory_after = process.memory_info().rss / 1024 / 1024  # MB\n                \n                inference_time = (end_time - start_time) * 1000  # Convert to ms\n                inference_times.append(inference_time)\n                memory_usage.append(memory_after - memory_before)\n            \n            if not inference_times:\n                continue\n            \n            # Calculate metrics\n            avg_inference_time = np.mean(inference_times)\n            p95_inference_time = np.percentile(inference_times, 95)\n            throughput = (batch_size * 1000) / avg_inference_time  # predictions per second\n            avg_memory = np.mean(memory_usage)\n            \n            # Get model memory usage\n            model_size_mb = self._get_model_memory_usage(model)\n            \n            # Calculate efficiency metrics\n            predictions_per_mb = throughput / max(model_size_mb, 1)\n            efficiency_score = self._calculate_efficiency_score(\n                avg_inference_time, model_size_mb, throughput\n            )\n            \n            metrics = ModelPerformanceMetrics(\n                model_name=self.model_name,\n                version=\"current\",\n                timestamp=datetime.now(),\n                avg_inference_time_ms=avg_inference_time,\n                p95_inference_time_ms=p95_inference_time,\n                throughput_predictions_per_sec=throughput,\n                model_memory_mb=model_size_mb,\n                inference_memory_mb=avg_memory,\n                peak_memory_mb=max(memory_usage) if memory_usage else 0,\n                batch_size=batch_size,\n                predictions_per_mb=predictions_per_mb,\n                efficiency_score=efficiency_score\n            )\n            \n            results[f\"batch_{batch_size}\"] = metrics\n            self.optimization_history.append(metrics)\n        \n        return results\n    \n    def _get_model_memory_usage(self, model: tf.keras.Model) -> float:\n        \"\"\"Estimate model memory usage in MB.\"\"\"\n        try:\n            # Calculate model size based on parameters\n            total_params = model.count_params()\n            \n            # Assume 4 bytes per parameter (float32)\n            model_size_bytes = total_params * 4\n            model_size_mb = model_size_bytes / (1024 * 1024)\n            \n            return model_size_mb\n            \n        except Exception as e:\n            logger.warning(f\"Could not calculate model memory usage: {e}\")\n            return 0.0\n    \n    def _calculate_efficiency_score(self, inference_time_ms: float, \n                                  model_size_mb: float, \n                                  throughput: float) -> float:\n        \"\"\"Calculate overall efficiency score (0-100).\"\"\"\n        # Normalize metrics and combine into efficiency score\n        # Lower inference time is better\n        time_score = max(0, 100 - inference_time_ms)  # Penalize slow inference\n        \n        # Lower memory usage is better\n        memory_score = max(0, 100 - model_size_mb)  # Penalize large models\n        \n        # Higher throughput is better\n        throughput_score = min(100, throughput / 10)  # Scale throughput\n        \n        # Weighted combination\n        efficiency_score = (time_score * 0.4 + memory_score * 0.3 + throughput_score * 0.3)\n        \n        return efficiency_score\n    \n    def auto_optimize(self, model: tf.keras.Model, \n                     test_data: np.ndarray,\n                     target_accuracy_threshold: float = 0.95) -> Dict[str, Any]:\n        \"\"\"Automatically apply and test optimization techniques.\"\"\"\n        logger.info(f\"Starting auto-optimization for {self.model_name}\")\n        \n        optimization_results = {\n            'original': None,\n            'quantized_dynamic': None,\n            'quantized_int8': None,\n            'pruned': None,\n            'optimized_inference': None,\n            'best_optimization': None\n        }\n        \n        # Benchmark original model\n        logger.info(\"Benchmarking original model...\")\n        original_results = self.benchmark_model(model, test_data, batch_sizes=[32])\n        optimization_results['original'] = original_results['batch_32']\n        \n        # Test quantization techniques\n        logger.info(\"Testing quantization...\")\n        try:\n            quantized_model = self.apply_quantization(model, \"dynamic\")\n            quantized_results = self.benchmark_model(quantized_model, test_data, batch_sizes=[32])\n            optimization_results['quantized_dynamic'] = quantized_results['batch_32']\n            optimization_results['quantized_dynamic'].quantization_applied = True\n            optimization_results['quantized_dynamic'].optimization_techniques.append(\"dynamic_quantization\")\n        except Exception as e:\n            logger.warning(f\"Dynamic quantization failed: {e}\")\n        \n        # Test pruning\n        logger.info(\"Testing pruning...\")\n        try:\n            pruned_model = self.apply_pruning(model, sparsity=0.5)\n            pruned_results = self.benchmark_model(pruned_model, test_data, batch_sizes=[32])\n            optimization_results['pruned'] = pruned_results['batch_32']\n            optimization_results['pruned'].pruning_applied = True\n            optimization_results['pruned'].optimization_techniques.append(\"magnitude_pruning\")\n        except Exception as e:\n            logger.warning(f\"Pruning failed: {e}\")\n        \n        # Test inference optimization\n        logger.info(\"Testing inference optimization...\")\n        try:\n            optimized_model = self.optimize_for_inference(model)\n            optimized_results = self.benchmark_model(optimized_model, test_data, batch_sizes=[32])\n            optimization_results['optimized_inference'] = optimized_results['batch_32']\n            optimization_results['optimized_inference'].optimization_techniques.append(\"inference_optimization\")\n        except Exception as e:\n            logger.warning(f\"Inference optimization failed: {e}\")\n        \n        # Find best optimization\n        best_optimization = self._find_best_optimization(optimization_results)\n        optimization_results['best_optimization'] = best_optimization\n        \n        logger.info(f\"Auto-optimization completed. Best: {best_optimization['name']}\")\n        \n        return optimization_results\n    \n    def _find_best_optimization(self, results: Dict[str, ModelPerformanceMetrics]) -> Dict[str, Any]:\n        \"\"\"Find the best optimization based on efficiency score.\"\"\"\n        best_score = -1\n        best_name = \"original\"\n        best_metrics = None\n        \n        for name, metrics in results.items():\n            if metrics is None or name == \"best_optimization\":\n                continue\n            \n            if metrics.efficiency_score > best_score:\n                best_score = metrics.efficiency_score\n                best_name = name\n                best_metrics = metrics\n        \n        return {\n            'name': best_name,\n            'metrics': best_metrics,\n            'efficiency_score': best_score\n        }\n\n\nclass BatchInferenceOptimizer:\n    \"\"\"Optimizer for batch inference performance.\"\"\"\n    \n    def __init__(self, model_name: str):\n        self.model_name = model_name\n        self.optimal_batch_sizes: Dict[str, int] = {}\n        self.batching_strategies = {\n            'static': self._static_batching,\n            'dynamic': self._dynamic_batching,\n            'adaptive': self._adaptive_batching\n        }\n    \n    def find_optimal_batch_size(self, model: tf.keras.Model, \n                               test_data: np.ndarray,\n                               max_batch_size: int = 128,\n                               memory_limit_mb: float = 1000) -> Dict[str, Any]:\n        \"\"\"Find optimal batch size for given constraints.\"\"\"\n        logger.info(f\"Finding optimal batch size for {self.model_name}\")\n        \n        batch_sizes = [2**i for i in range(0, int(np.log2(max_batch_size)) + 1)]\n        results = []\n        \n        for batch_size in batch_sizes:\n            try:\n                # Test batch processing\n                test_batch = test_data[:batch_size]\n                \n                # Monitor memory usage\n                import psutil\n                process = psutil.Process()\n                memory_before = process.memory_info().rss / 1024 / 1024\n                \n                # Time inference\n                start_time = time.perf_counter()\n                predictions = model.predict(test_batch, verbose=0)\n                end_time = time.perf_counter()\n                \n                memory_after = process.memory_info().rss / 1024 / 1024\n                memory_used = memory_after - memory_before\n                \n                # Check memory constraint\n                if memory_used > memory_limit_mb:\n                    logger.warning(f\"Batch size {batch_size} exceeds memory limit ({memory_used:.1f}MB)\")\n                    break\n                \n                inference_time = end_time - start_time\n                throughput = batch_size / inference_time\n                latency_per_item = (inference_time * 1000) / batch_size  # ms per item\n                \n                results.append({\n                    'batch_size': batch_size,\n                    'inference_time_s': inference_time,\n                    'throughput_items_per_s': throughput,\n                    'latency_per_item_ms': latency_per_item,\n                    'memory_used_mb': memory_used,\n                    'efficiency_score': throughput / max(memory_used, 1)\n                })\n                \n            except Exception as e:\n                logger.warning(f\"Batch size {batch_size} failed: {e}\")\n                break\n        \n        if not results:\n            return {'error': 'No valid batch sizes found'}\n        \n        # Find optimal batch size based on efficiency\n        optimal_result = max(results, key=lambda x: x['efficiency_score'])\n        optimal_batch_size = optimal_result['batch_size']\n        \n        self.optimal_batch_sizes[self.model_name] = optimal_batch_size\n        \n        logger.info(\n            f\"Optimal batch size for {self.model_name}: {optimal_batch_size} \"\n            f\"(throughput: {optimal_result['throughput_items_per_s']:.1f} items/s)\"\n        )\n        \n        return {\n            'optimal_batch_size': optimal_batch_size,\n            'optimal_result': optimal_result,\n            'all_results': results\n        }\n    \n    def _static_batching(self, requests: List[Any], batch_size: int) -> List[List[Any]]:\n        \"\"\"Static batching strategy.\"\"\"\n        batches = []\n        for i in range(0, len(requests), batch_size):\n            batches.append(requests[i:i + batch_size])\n        return batches\n    \n    def _dynamic_batching(self, requests: List[Any], \n                         max_batch_size: int,\n                         max_wait_time_ms: float = 100) -> List[List[Any]]:\n        \"\"\"Dynamic batching with timeout.\"\"\"\n        # Simplified implementation - in production, this would be more sophisticated\n        if len(requests) >= max_batch_size:\n            return self._static_batching(requests, max_batch_size)\n        else:\n            # In real implementation, would wait for more requests or timeout\n            return [requests] if requests else []\n    \n    def _adaptive_batching(self, requests: List[Any], \n                          current_load: float = 0.5) -> List[List[Any]]:\n        \"\"\"Adaptive batching based on system load.\"\"\"\n        # Adjust batch size based on current system load\n        optimal_batch_size = self.optimal_batch_sizes.get(self.model_name, 32)\n        \n        if current_load > 0.8:  # High load, use smaller batches\n            batch_size = max(1, optimal_batch_size // 2)\n        elif current_load < 0.3:  # Low load, use larger batches\n            batch_size = min(64, optimal_batch_size * 2)\n        else:\n            batch_size = optimal_batch_size\n        \n        return self._static_batching(requests, batch_size)\n\n\nclass ModelCacheOptimizer:\n    \"\"\"Optimizer for ML model inference caching.\"\"\"\n    \n    def __init__(self, cache_size_mb: float = 500):\n        self.cache_size_mb = cache_size_mb\n        self.prediction_cache: Dict[str, Any] = {}\n        self.cache_stats = {\n            'hits': 0,\n            'misses': 0,\n            'evictions': 0\n        }\n        self.cache_times: List[float] = []\n    \n    def get_cache_key(self, model_name: str, input_data: np.ndarray, \n                     model_version: str = \"v1\") -> str:\n        \"\"\"Generate cache key for model input.\"\"\"\n        # Create hash of input data\n        input_hash = hashlib.md5(input_data.tobytes()).hexdigest()[:16]\n        cache_key = f\"{model_name}:{model_version}:{input_hash}\"\n        return cache_key\n    \n    def get_cached_prediction(self, cache_key: str) -> Optional[np.ndarray]:\n        \"\"\"Get cached prediction if available.\"\"\"\n        start_time = time.perf_counter()\n        \n        if cache_key in self.prediction_cache:\n            result = self.prediction_cache[cache_key]['prediction']\n            self.cache_stats['hits'] += 1\n            \n            # Update access time for LRU\n            self.prediction_cache[cache_key]['last_access'] = time.time()\n            \n            cache_time = (time.perf_counter() - start_time) * 1000\n            self.cache_times.append(cache_time)\n            \n            return result\n        else:\n            self.cache_stats['misses'] += 1\n            return None\n    \n    def cache_prediction(self, cache_key: str, prediction: np.ndarray):\n        \"\"\"Cache model prediction.\"\"\"\n        # Check cache size and evict if necessary\n        self._evict_if_necessary()\n        \n        self.prediction_cache[cache_key] = {\n            'prediction': prediction,\n            'cached_at': time.time(),\n            'last_access': time.time(),\n            'size_bytes': prediction.nbytes\n        }\n    \n    def _evict_if_necessary(self):\n        \"\"\"Evict oldest entries if cache is full.\"\"\"\n        current_size_bytes = sum(\n            entry['size_bytes'] \n            for entry in self.prediction_cache.values()\n        )\n        \n        max_size_bytes = self.cache_size_mb * 1024 * 1024\n        \n        while current_size_bytes > max_size_bytes and self.prediction_cache:\n            # Find least recently used entry\n            lru_key = min(\n                self.prediction_cache.keys(),\n                key=lambda k: self.prediction_cache[k]['last_access']\n            )\n            \n            # Remove LRU entry\n            removed_entry = self.prediction_cache.pop(lru_key)\n            current_size_bytes -= removed_entry['size_bytes']\n            self.cache_stats['evictions'] += 1\n    \n    def get_cache_statistics(self) -> Dict[str, Any]:\n        \"\"\"Get cache performance statistics.\"\"\"\n        total_requests = self.cache_stats['hits'] + self.cache_stats['misses']\n        hit_rate = self.cache_stats['hits'] / max(total_requests, 1)\n        \n        avg_cache_time = np.mean(self.cache_times) if self.cache_times else 0\n        \n        cache_size_bytes = sum(\n            entry['size_bytes'] \n            for entry in self.prediction_cache.values()\n        )\n        \n        return {\n            'hit_rate': hit_rate,\n            'total_requests': total_requests,\n            'hits': self.cache_stats['hits'],\n            'misses': self.cache_stats['misses'],\n            'evictions': self.cache_stats['evictions'],\n            'avg_cache_retrieval_time_ms': avg_cache_time,\n            'cache_size_mb': cache_size_bytes / (1024 * 1024),\n            'cached_entries': len(self.prediction_cache)\n        }\n\n\nclass MLPerformanceOptimizer:\n    \"\"\"Comprehensive ML performance optimization manager.\"\"\"\n    \n    def __init__(self):\n        self.model_optimizers: Dict[str, TensorFlowOptimizer] = {}\n        self.batch_optimizer = BatchInferenceOptimizer(\"global\")\n        self.cache_optimizer = ModelCacheOptimizer()\n        self.optimization_results: List[Dict[str, Any]] = []\n    \n    def register_model(self, model_name: str, model: tf.keras.Model):\n        \"\"\"Register a model for optimization.\"\"\"\n        self.model_optimizers[model_name] = TensorFlowOptimizer(model, model_name)\n        logger.info(f\"Registered model for optimization: {model_name}\")\n    \n    def optimize_model(self, model_name: str, test_data: np.ndarray) -> Dict[str, Any]:\n        \"\"\"Run comprehensive optimization for a model.\"\"\"\n        if model_name not in self.model_optimizers:\n            return {'error': f'Model {model_name} not registered'}\n        \n        optimizer = self.model_optimizers[model_name]\n        \n        logger.info(f\"Starting comprehensive optimization for {model_name}\")\n        \n        # Run auto-optimization\n        optimization_results = optimizer.auto_optimize(\n            optimizer.original_model, \n            test_data\n        )\n        \n        # Find optimal batch size\n        batch_optimization = self.batch_optimizer.find_optimal_batch_size(\n            optimizer.original_model,\n            test_data\n        )\n        \n        # Combine results\n        comprehensive_results = {\n            'model_name': model_name,\n            'timestamp': datetime.now().isoformat(),\n            'model_optimizations': optimization_results,\n            'batch_optimization': batch_optimization,\n            'cache_statistics': self.cache_optimizer.get_cache_statistics(),\n            'recommendations': self._generate_recommendations(\n                model_name, optimization_results, batch_optimization\n            )\n        }\n        \n        self.optimization_results.append(comprehensive_results)\n        \n        logger.info(f\"Comprehensive optimization completed for {model_name}\")\n        return comprehensive_results\n    \n    def _generate_recommendations(self, model_name: str, \n                                model_opts: Dict[str, Any],\n                                batch_opts: Dict[str, Any]) -> List[str]:\n        \"\"\"Generate optimization recommendations.\"\"\"\n        recommendations = []\n        \n        # Model optimization recommendations\n        best_opt = model_opts.get('best_optimization')\n        if best_opt and best_opt['name'] != 'original':\n            recommendations.append(\n                f\"Apply {best_opt['name']} optimization for {model_name} \"\n                f\"(efficiency improvement: {best_opt['efficiency_score']:.1f})\"\n            )\n        \n        # Batch size recommendations\n        if 'optimal_batch_size' in batch_opts:\n            optimal_batch = batch_opts['optimal_batch_size']\n            recommendations.append(\n                f\"Use batch size {optimal_batch} for optimal throughput\"\n            )\n        \n        # Cache recommendations\n        cache_stats = self.cache_optimizer.get_cache_statistics()\n        if cache_stats['hit_rate'] < 0.5:\n            recommendations.append(\n                \"Low cache hit rate. Consider adjusting cache size or TTL strategy\"\n            )\n        \n        return recommendations\n    \n    def export_optimization_report(self, filename: str):\n        \"\"\"Export comprehensive optimization report.\"\"\"\n        report = {\n            'export_timestamp': datetime.now().isoformat(),\n            'optimization_results': self.optimization_results,\n            'global_cache_statistics': self.cache_optimizer.get_cache_statistics(),\n            'summary': self._generate_summary()\n        }\n        \n        with open(filename, 'w') as f:\n            json.dump(report, f, indent=2, default=str)\n        \n        logger.info(f\"ML optimization report exported to {filename}\")\n    \n    def _generate_summary(self) -> Dict[str, Any]:\n        \"\"\"Generate optimization summary.\"\"\"\n        if not self.optimization_results:\n            return {'message': 'No optimization results available'}\n        \n        total_models = len(set(\n            result['model_name'] \n            for result in self.optimization_results\n        ))\n        \n        # Calculate average improvements\n        improvements = []\n        for result in self.optimization_results:\n            best_opt = result['model_optimizations'].get('best_optimization')\n            if best_opt and best_opt['name'] != 'original':\n                improvements.append(best_opt['efficiency_score'])\n        \n        avg_improvement = np.mean(improvements) if improvements else 0\n        \n        return {\n            'total_models_optimized': total_models,\n            'optimization_sessions': len(self.optimization_results),\n            'average_efficiency_improvement': avg_improvement,\n            'models_with_improvements': len(improvements),\n            'cache_performance': self.cache_optimizer.get_cache_statistics()\n        }\n\n\nif __name__ == \"__main__\":\n    # Example usage\n    import tensorflow as tf\n    \n    # Create a simple model for testing\n    model = tf.keras.Sequential([\n        tf.keras.layers.Dense(64, activation='relu', input_shape=(10,)),\n        tf.keras.layers.Dense(32, activation='relu'),\n        tf.keras.layers.Dense(1, activation='sigmoid')\n    ])\n    \n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    \n    # Generate test data\n    test_data = np.random.random((1000, 10))\n    \n    # Create optimizer and run optimization\n    ml_optimizer = MLPerformanceOptimizer()\n    ml_optimizer.register_model(\"test_model\", model)\n    \n    try:\n        results = ml_optimizer.optimize_model(\"test_model\", test_data)\n        print(\"Optimization results:\")\n        print(json.dumps(results, indent=2, default=str))\n        \n    except Exception as e:\n        print(f\"Optimization failed: {e}\")\n        \n        # Show expected output structure\n        example_results = {\n            'model_name': 'test_model',\n            'timestamp': datetime.now().isoformat(),\n            'recommendations': [\n                'Apply quantization for memory efficiency',\n                'Use batch size 32 for optimal throughput',\n                'Consider model pruning for production deployment'\n            ]\n        }\n        \n        print(\"\\nExample optimization structure:\")\n        print(json.dumps(example_results, indent=2))"