"""
Scalability testing suite for the rental ML system.

This module provides comprehensive scalability testing including horizontal scaling,
auto-scaling validation, load balancer testing, and multi-region deployment validation.
"""

import asyncio
import aiohttp
import time
import logging
import json
import docker
import kubernetes
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, field
import numpy as np
import psutil
from pathlib import Path
import subprocess
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed

from ..performance.utils.performance_config import (
    PerformanceConfig, PerformanceMetrics, PerformanceTimer,
    performance_monitor, measure_performance
)

logger = logging.getLogger(__name__)


@dataclass
class ScalabilityTestConfig:
    \"\"\"Configuration for scalability tests.\"\"\"\n    \n    # Scaling parameters\n    min_instances: int = 1\n    max_instances: int = 10\n    scale_step_size: int = 1\n    scale_up_threshold: float = 70.0  # CPU percent\n    scale_down_threshold: float = 30.0  # CPU percent\n    \n    # Load parameters\n    base_load_rps: float = 10.0  # Requests per second\n    max_load_rps: float = 1000.0\n    load_ramp_duration_seconds: int = 300  # 5 minutes\n    \n    # Test duration\n    test_duration_seconds: int = 1800  # 30 minutes\n    warmup_duration_seconds: int = 300  # 5 minutes\n    cooldown_duration_seconds: int = 300  # 5 minutes\n    \n    # Target endpoints\n    target_endpoints: List[str] = field(default_factory=lambda: [\n        \"/api/v1/search\",\n        \"/api/v1/recommendations/user/1\",\n        \"/api/v1/properties\",\n        \"/health\"\n    ])\n    \n    # Performance thresholds\n    max_response_time_ms: float = 2000.0\n    max_error_rate_percent: float = 1.0\n    min_throughput_rps: float = 5.0\n    \n    # Kubernetes/Docker configuration\n    k8s_namespace: str = \"rental-ml-system\"\n    k8s_deployment_name: str = \"rental-ml-api\"\n    docker_image: str = \"rental-ml-system:latest\"\n    \n    def to_dict(self) -> Dict[str, Any]:\n        return {\n            'min_instances': self.min_instances,\n            'max_instances': self.max_instances,\n            'scale_step_size': self.scale_step_size,\n            'scale_up_threshold': self.scale_up_threshold,\n            'scale_down_threshold': self.scale_down_threshold,\n            'base_load_rps': self.base_load_rps,\n            'max_load_rps': self.max_load_rps,\n            'test_duration_seconds': self.test_duration_seconds,\n            'target_endpoints': self.target_endpoints,\n            'max_response_time_ms': self.max_response_time_ms,\n            'max_error_rate_percent': self.max_error_rate_percent,\n            'min_throughput_rps': self.min_throughput_rps\n        }\n\n\n@dataclass\nclass ScalabilityTestResult:\n    \"\"\"Results from scalability testing.\"\"\"\n    \n    test_name: str\n    config: ScalabilityTestConfig\n    start_time: datetime\n    end_time: datetime\n    \n    # Scaling results\n    instance_count_history: List[Tuple[datetime, int]]\n    scaling_events: List[Dict[str, Any]]\n    \n    # Performance results\n    response_time_history: List[Tuple[datetime, float]]\n    throughput_history: List[Tuple[datetime, float]]\n    error_rate_history: List[Tuple[datetime, float]]\n    \n    # Resource usage\n    cpu_usage_history: List[Tuple[datetime, float]]\n    memory_usage_history: List[Tuple[datetime, float]]\n    \n    # Summary statistics\n    avg_response_time_ms: float = 0.0\n    p95_response_time_ms: float = 0.0\n    avg_throughput_rps: float = 0.0\n    avg_error_rate_percent: float = 0.0\n    total_scaling_events: int = 0\n    scaling_effectiveness_score: float = 0.0\n    \n    def calculate_summary(self):\n        \"\"\"Calculate summary statistics from historical data.\"\"\"\n        if self.response_time_history:\n            response_times = [rt[1] for rt in self.response_time_history]\n            self.avg_response_time_ms = np.mean(response_times)\n            self.p95_response_time_ms = np.percentile(response_times, 95)\n        \n        if self.throughput_history:\n            throughputs = [th[1] for th in self.throughput_history]\n            self.avg_throughput_rps = np.mean(throughputs)\n        \n        if self.error_rate_history:\n            error_rates = [er[1] for er in self.error_rate_history]\n            self.avg_error_rate_percent = np.mean(error_rates)\n        \n        self.total_scaling_events = len(self.scaling_events)\n        self.scaling_effectiveness_score = self._calculate_effectiveness_score()\n    \n    def _calculate_effectiveness_score(self) -> float:\n        \"\"\"Calculate scaling effectiveness score (0-100).\"\"\"\n        score = 100.0\n        \n        # Penalize high response times\n        if self.avg_response_time_ms > self.config.max_response_time_ms:\n            score -= 20\n        \n        # Penalize high error rates\n        if self.avg_error_rate_percent > self.config.max_error_rate_percent:\n            score -= 30\n        \n        # Penalize low throughput\n        if self.avg_throughput_rps < self.config.min_throughput_rps:\n            score -= 25\n        \n        # Penalize excessive scaling events (indicates instability)\n        expected_scaling_events = 5  # Reasonable number for a test\n        if self.total_scaling_events > expected_scaling_events * 2:\n            score -= 15\n        \n        return max(0, score)\n    \n    def to_dict(self) -> Dict[str, Any]:\n        return {\n            'test_name': self.test_name,\n            'config': self.config.to_dict(),\n            'start_time': self.start_time.isoformat(),\n            'end_time': self.end_time.isoformat(),\n            'duration_seconds': (self.end_time - self.start_time).total_seconds(),\n            'instance_count_history': [(t.isoformat(), c) for t, c in self.instance_count_history],\n            'scaling_events': self.scaling_events,\n            'avg_response_time_ms': self.avg_response_time_ms,\n            'p95_response_time_ms': self.p95_response_time_ms,\n            'avg_throughput_rps': self.avg_throughput_rps,\n            'avg_error_rate_percent': self.avg_error_rate_percent,\n            'total_scaling_events': self.total_scaling_events,\n            'scaling_effectiveness_score': self.scaling_effectiveness_score\n        }\n\n\nclass KubernetesScaler:\n    \"\"\"Kubernetes-based scaling operations.\"\"\"\n    \n    def __init__(self, namespace: str = \"default\"):\n        self.namespace = namespace\n        self.k8s_client = None\n        self._initialize_client()\n    \n    def _initialize_client(self):\n        \"\"\"Initialize Kubernetes client.\"\"\"\n        try:\n            kubernetes.config.load_incluster_config()\n            logger.info(\"Loaded in-cluster Kubernetes config\")\n        except kubernetes.config.ConfigException:\n            try:\n                kubernetes.config.load_kube_config()\n                logger.info(\"Loaded local Kubernetes config\")\n            except kubernetes.config.ConfigException:\n                logger.warning(\"Could not load Kubernetes config. Scaling tests may fail.\")\n                return\n        \n        self.k8s_client = kubernetes.client.AppsV1Api()\n    \n    async def get_deployment_replicas(self, deployment_name: str) -> int:\n        \"\"\"Get current number of replicas for a deployment.\"\"\"\n        if not self.k8s_client:\n            return 1  # Default assumption\n        \n        try:\n            deployment = self.k8s_client.read_namespaced_deployment(\n                name=deployment_name,\n                namespace=self.namespace\n            )\n            return deployment.spec.replicas or 1\n        except Exception as e:\n            logger.error(f\"Error getting deployment replicas: {e}\")\n            return 1\n    \n    async def scale_deployment(self, deployment_name: str, replicas: int) -> bool:\n        \"\"\"Scale a deployment to the specified number of replicas.\"\"\"\n        if not self.k8s_client:\n            logger.warning(\"Kubernetes client not available. Simulating scaling.\")\n            await asyncio.sleep(1)  # Simulate scaling time\n            return True\n        \n        try:\n            # Update deployment replicas\n            body = kubernetes.client.V1Scale(\n                metadata=kubernetes.client.V1ObjectMeta(name=deployment_name),\n                spec=kubernetes.client.V1ScaleSpec(replicas=replicas)\n            )\n            \n            self.k8s_client.patch_namespaced_deployment_scale(\n                name=deployment_name,\n                namespace=self.namespace,\n                body=body\n            )\n            \n            logger.info(f\"Scaled deployment {deployment_name} to {replicas} replicas\")\n            return True\n            \n        except Exception as e:\n            logger.error(f\"Error scaling deployment: {e}\")\n            return False\n    \n    async def wait_for_scaling(self, deployment_name: str, target_replicas: int, \n                              timeout_seconds: int = 300) -> bool:\n        \"\"\"Wait for deployment to reach target replica count.\"\"\"\n        start_time = time.time()\n        \n        while time.time() - start_time < timeout_seconds:\n            current_replicas = await self.get_deployment_replicas(deployment_name)\n            \n            if current_replicas == target_replicas:\n                logger.info(f\"Deployment {deployment_name} successfully scaled to {target_replicas}\")\n                return True\n            \n            await asyncio.sleep(5)  # Check every 5 seconds\n        \n        logger.warning(f\"Timeout waiting for deployment {deployment_name} to scale to {target_replicas}\")\n        return False\n    \n    async def get_pod_metrics(self, deployment_name: str) -> Dict[str, float]:\n        \"\"\"Get aggregated metrics for pods in a deployment.\"\"\"\n        # This would typically integrate with Kubernetes metrics server\n        # For now, return mock data\n        return {\n            'cpu_usage_percent': np.random.uniform(20, 80),\n            'memory_usage_percent': np.random.uniform(30, 70),\n            'network_io_bytes_per_sec': np.random.uniform(1000, 10000)\n        }\n\n\nclass DockerScaler:\n    \"\"\"Docker-based scaling operations for local testing.\"\"\"\n    \n    def __init__(self, image_name: str, network_name: str = \"rental-ml-network\"):\n        self.image_name = image_name\n        self.network_name = network_name\n        self.docker_client = None\n        self.containers: Dict[str, Any] = {}  # container_name -> container object\n        self._initialize_client()\n    \n    def _initialize_client(self):\n        \"\"\"Initialize Docker client.\"\"\"\n        try:\n            self.docker_client = docker.from_env()\n            logger.info(\"Docker client initialized\")\n        except Exception as e:\n            logger.error(f\"Failed to initialize Docker client: {e}\")\n    \n    async def get_container_count(self, label_filter: str = \"app=rental-ml\") -> int:\n        \"\"\"Get current number of running containers.\"\"\"\n        if not self.docker_client:\n            return 1\n        \n        try:\n            containers = self.docker_client.containers.list(\n                filters={\"label\": label_filter, \"status\": \"running\"}\n            )\n            return len(containers)\n        except Exception as e:\n            logger.error(f\"Error getting container count: {e}\")\n            return 1\n    \n    async def scale_containers(self, target_count: int, \n                              base_port: int = 8000) -> bool:\n        \"\"\"Scale containers to target count.\"\"\"\n        if not self.docker_client:\n            logger.warning(\"Docker client not available. Simulating scaling.\")\n            return True\n        \n        try:\n            current_count = await self.get_container_count()\n            \n            if target_count > current_count:\n                # Scale up\n                for i in range(current_count, target_count):\n                    container_name = f\"rental-ml-{i+1}\"\n                    port = base_port + i\n                    \n                    container = self.docker_client.containers.run(\n                        self.image_name,\n                        name=container_name,\n                        ports={f'{base_port}/tcp': port},\n                        network=self.network_name,\n                        labels={\"app\": \"rental-ml\", \"instance\": str(i+1)},\n                        detach=True,\n                        environment={\n                            \"PORT\": str(base_port),\n                            \"INSTANCE_ID\": str(i+1)\n                        }\n                    )\n                    \n                    self.containers[container_name] = container\n                    logger.info(f\"Started container {container_name} on port {port}\")\n            \n            elif target_count < current_count:\n                # Scale down\n                containers_to_remove = list(self.containers.keys())[target_count:]\n                \n                for container_name in containers_to_remove:\n                    if container_name in self.containers:\n                        container = self.containers[container_name]\n                        container.stop(timeout=10)\n                        container.remove()\n                        del self.containers[container_name]\n                        logger.info(f\"Stopped and removed container {container_name}\")\n            \n            return True\n            \n        except Exception as e:\n            logger.error(f\"Error scaling containers: {e}\")\n            return False\n    \n    async def cleanup_containers(self):\n        \"\"\"Clean up all managed containers.\"\"\"\n        for container_name, container in self.containers.items():\n            try:\n                container.stop(timeout=10)\n                container.remove()\n                logger.info(f\"Cleaned up container {container_name}\")\n            except Exception as e:\n                logger.warning(f\"Error cleaning up container {container_name}: {e}\")\n        \n        self.containers.clear()\n\n\nclass LoadGenerator:\n    \"\"\"Generate load for scalability testing.\"\"\"\n    \n    def __init__(self, base_url: str = \"http://localhost:8000\"):\n        self.base_url = base_url\n        self.session: Optional[aiohttp.ClientSession] = None\n        self.load_active = False\n        self.current_rps = 0.0\n        \n        # Metrics tracking\n        self.request_times: List[float] = []\n        self.error_count = 0\n        self.success_count = 0\n        self.requests_sent = 0\n    \n    async def start_session(self):\n        \"\"\"Start HTTP session.\"\"\"\n        if not self.session:\n            timeout = aiohttp.ClientTimeout(total=30)\n            self.session = aiohttp.ClientSession(timeout=timeout)\n    \n    async def stop_session(self):\n        \"\"\"Stop HTTP session.\"\"\"\n        if self.session:\n            await self.session.close()\n            self.session = None\n    \n    async def generate_load(self, target_rps: float, duration_seconds: int, \n                           endpoints: List[str]):\n        \"\"\"Generate load at specified RPS for given duration.\"\"\"\n        if not self.session:\n            await self.start_session()\n        \n        self.load_active = True\n        self.current_rps = target_rps\n        \n        # Calculate request interval\n        request_interval = 1.0 / target_rps if target_rps > 0 else 1.0\n        \n        start_time = time.time()\n        end_time = start_time + duration_seconds\n        \n        logger.info(f\"Starting load generation: {target_rps} RPS for {duration_seconds}s\")\n        \n        while time.time() < end_time and self.load_active:\n            # Send request\n            endpoint = np.random.choice(endpoints)\n            await self._send_request(endpoint)\n            \n            # Wait for next request\n            await asyncio.sleep(request_interval)\n        \n        logger.info(f\"Load generation completed. Sent {self.requests_sent} requests\")\n    \n    async def _send_request(self, endpoint: str):\n        \"\"\"Send a single request and record metrics.\"\"\"\n        url = f\"{self.base_url}{endpoint}\"\n        start_time = time.perf_counter()\n        \n        try:\n            async with self.session.get(url) as response:\n                await response.text()  # Consume response body\n                \n                response_time = (time.perf_counter() - start_time) * 1000  # ms\n                self.request_times.append(response_time)\n                \n                if 200 <= response.status < 400:\n                    self.success_count += 1\n                else:\n                    self.error_count += 1\n                \n        except Exception as e:\n            response_time = (time.perf_counter() - start_time) * 1000  # ms\n            self.request_times.append(response_time)\n            self.error_count += 1\n            logger.debug(f\"Request error: {e}\")\n        \n        self.requests_sent += 1\n    \n    def get_metrics(self) -> Dict[str, float]:\n        \"\"\"Get current load generator metrics.\"\"\"\n        total_requests = self.success_count + self.error_count\n        \n        return {\n            'requests_sent': self.requests_sent,\n            'success_count': self.success_count,\n            'error_count': self.error_count,\n            'error_rate_percent': (self.error_count / max(total_requests, 1)) * 100,\n            'avg_response_time_ms': np.mean(self.request_times) if self.request_times else 0,\n            'p95_response_time_ms': np.percentile(self.request_times, 95) if self.request_times else 0,\n            'current_rps': self.current_rps\n        }\n    \n    def reset_metrics(self):\n        \"\"\"Reset metrics for new test phase.\"\"\"\n        self.request_times.clear()\n        self.error_count = 0\n        self.success_count = 0\n        self.requests_sent = 0\n\n\nclass ScalabilityTestRunner:\n    \"\"\"Main scalability test execution engine.\"\"\"\n    \n    def __init__(self, config: ScalabilityTestConfig, use_kubernetes: bool = True):\n        self.config = config\n        self.use_kubernetes = use_kubernetes\n        \n        # Initialize scalers\n        if use_kubernetes:\n            self.scaler = KubernetesScaler(config.k8s_namespace)\n        else:\n            self.scaler = DockerScaler(config.docker_image)\n        \n        self.load_generator = LoadGenerator()\n        \n        # Test state\n        self.test_active = False\n        self.current_instances = config.min_instances\n        \n        # Results tracking\n        self.test_results: Optional[ScalabilityTestResult] = None\n    \n    async def run_horizontal_scaling_test(self) -> ScalabilityTestResult:\n        \"\"\"Run horizontal scaling test with gradual load increase.\"\"\"\n        logger.info(\"Starting horizontal scaling test\")\n        \n        start_time = datetime.now()\n        self.test_active = True\n        \n        # Initialize test results\n        self.test_results = ScalabilityTestResult(\n            test_name=\"horizontal_scaling_test\",\n            config=self.config,\n            start_time=start_time,\n            end_time=start_time,  # Will be updated\n            instance_count_history=[],\n            scaling_events=[],\n            response_time_history=[],\n            throughput_history=[],\n            error_rate_history=[],\n            cpu_usage_history=[],\n            memory_usage_history=[]\n        )\n        \n        try:\n            # Start with minimum instances\n            await self._scale_to_instances(self.config.min_instances, \"initial_setup\")\n            \n            # Start load generator session\n            await self.load_generator.start_session()\n            \n            # Start monitoring\n            monitor_task = asyncio.create_task(self._monitoring_loop())\n            \n            # Phase 1: Warmup\n            logger.info(\"Phase 1: Warmup\")\n            await self._warmup_phase()\n            \n            # Phase 2: Gradual load increase with auto-scaling\n            logger.info(\"Phase 2: Gradual load increase\")\n            await self._load_ramp_phase()\n            \n            # Phase 3: Sustained high load\n            logger.info(\"Phase 3: Sustained high load\")\n            await self._sustained_load_phase()\n            \n            # Phase 4: Load decrease and scale down\n            logger.info(\"Phase 4: Load decrease\")\n            await self._scale_down_phase()\n            \n            # Phase 5: Cooldown\n            logger.info(\"Phase 5: Cooldown\")\n            await self._cooldown_phase()\n            \n        finally:\n            # Cleanup\n            self.test_active = False\n            monitor_task.cancel()\n            \n            try:\n                await monitor_task\n            except asyncio.CancelledError:\n                pass\n            \n            await self.load_generator.stop_session()\n            \n            # Cleanup containers if using Docker\n            if not self.use_kubernetes and hasattr(self.scaler, 'cleanup_containers'):\n                await self.scaler.cleanup_containers()\n        \n        # Finalize results\n        self.test_results.end_time = datetime.now()\n        self.test_results.calculate_summary()\n        \n        logger.info(\n            f\"Horizontal scaling test completed. \"\n            f\"Effectiveness score: {self.test_results.scaling_effectiveness_score:.1f}\"\n        )\n        \n        return self.test_results\n    \n    async def _warmup_phase(self):\n        \"\"\"Warmup phase with light load.\"\"\"\n        warmup_rps = self.config.base_load_rps\n        await self.load_generator.generate_load(\n            target_rps=warmup_rps,\n            duration_seconds=self.config.warmup_duration_seconds,\n            endpoints=self.config.target_endpoints\n        )\n    \n    async def _load_ramp_phase(self):\n        \"\"\"Gradual load increase phase.\"\"\"\n        ramp_steps = 10\n        step_duration = self.config.load_ramp_duration_seconds // ramp_steps\n        \n        for step in range(ramp_steps):\n            # Calculate target RPS for this step\n            progress = (step + 1) / ramp_steps\n            target_rps = self.config.base_load_rps + (\n                (self.config.max_load_rps - self.config.base_load_rps) * progress\n            )\n            \n            logger.info(f\"Ramp step {step + 1}/{ramp_steps}: {target_rps:.1f} RPS\")\n            \n            # Generate load for this step\n            load_task = asyncio.create_task(\n                self.load_generator.generate_load(\n                    target_rps=target_rps,\n                    duration_seconds=step_duration,\n                    endpoints=self.config.target_endpoints\n                )\n            )\n            \n            # Check if scaling is needed during this step\n            scale_check_task = asyncio.create_task(\n                self._check_and_scale_during_load(step_duration)\n            )\n            \n            await asyncio.gather(load_task, scale_check_task)\n    \n    async def _sustained_load_phase(self):\n        \"\"\"Sustained high load phase.\"\"\"\n        sustained_duration = self.config.test_duration_seconds - self.config.load_ramp_duration_seconds\n        \n        # Generate sustained high load\n        load_task = asyncio.create_task(\n            self.load_generator.generate_load(\n                target_rps=self.config.max_load_rps,\n                duration_seconds=sustained_duration,\n                endpoints=self.config.target_endpoints\n            )\n        )\n        \n        # Continue monitoring and scaling\n        scale_task = asyncio.create_task(\n            self._check_and_scale_during_load(sustained_duration)\n        )\n        \n        await asyncio.gather(load_task, scale_task)\n    \n    async def _scale_down_phase(self):\n        \"\"\"Scale down phase with decreasing load.\"\"\"\n        scale_down_steps = 5\n        step_duration = 60  # 1 minute per step\n        \n        for step in range(scale_down_steps):\n            # Calculate decreasing RPS\n            progress = (step + 1) / scale_down_steps\n            target_rps = self.config.max_load_rps * (1 - progress) + self.config.base_load_rps * progress\n            \n            logger.info(f\"Scale down step {step + 1}/{scale_down_steps}: {target_rps:.1f} RPS\")\n            \n            # Generate decreasing load\n            load_task = asyncio.create_task(\n                self.load_generator.generate_load(\n                    target_rps=target_rps,\n                    duration_seconds=step_duration,\n                    endpoints=self.config.target_endpoints\n                )\n            )\n            \n            # Check for scale down opportunities\n            scale_task = asyncio.create_task(\n                self._check_and_scale_during_load(step_duration)\n            )\n            \n            await asyncio.gather(load_task, scale_task)\n    \n    async def _cooldown_phase(self):\n        \"\"\"Cooldown phase with minimal load.\"\"\"\n        await self.load_generator.generate_load(\n            target_rps=self.config.base_load_rps,\n            duration_seconds=self.config.cooldown_duration_seconds,\n            endpoints=self.config.target_endpoints\n        )\n    \n    async def _check_and_scale_during_load(self, duration_seconds: int):\n        \"\"\"Check metrics and scale if needed during load generation.\"\"\"\n        check_interval = 30  # Check every 30 seconds\n        checks = duration_seconds // check_interval\n        \n        for _ in range(checks):\n            await asyncio.sleep(check_interval)\n            \n            if not self.test_active:\n                break\n            \n            # Get current metrics\n            load_metrics = self.load_generator.get_metrics()\n            \n            # Simulate CPU usage based on load (in real scenario, get from monitoring)\n            simulated_cpu = min(90, 20 + (load_metrics['current_rps'] / 10))\n            \n            # Decide if scaling is needed\n            should_scale_up = (\n                simulated_cpu > self.config.scale_up_threshold and\n                self.current_instances < self.config.max_instances and\n                load_metrics['avg_response_time_ms'] > self.config.max_response_time_ms * 0.7\n            )\n            \n            should_scale_down = (\n                simulated_cpu < self.config.scale_down_threshold and\n                self.current_instances > self.config.min_instances and\n                load_metrics['avg_response_time_ms'] < self.config.max_response_time_ms * 0.3\n            )\n            \n            if should_scale_up:\n                new_instances = min(\n                    self.current_instances + self.config.scale_step_size,\n                    self.config.max_instances\n                )\n                await self._scale_to_instances(new_instances, \"scale_up_cpu_threshold\")\n            \n            elif should_scale_down:\n                new_instances = max(\n                    self.current_instances - self.config.scale_step_size,\n                    self.config.min_instances\n                )\n                await self._scale_to_instances(new_instances, \"scale_down_cpu_threshold\")\n    \n    async def _scale_to_instances(self, target_instances: int, reason: str) -> bool:\n        \"\"\"Scale to target number of instances.\"\"\"\n        if target_instances == self.current_instances:\n            return True\n        \n        logger.info(f\"Scaling from {self.current_instances} to {target_instances} instances (reason: {reason})\")\n        \n        # Record scaling event\n        scaling_event = {\n            'timestamp': datetime.now().isoformat(),\n            'from_instances': self.current_instances,\n            'to_instances': target_instances,\n            'reason': reason,\n            'duration_seconds': 0  # Will be updated\n        }\n        \n        start_time = time.time()\n        \n        # Perform scaling\n        if self.use_kubernetes:\n            success = await self.scaler.scale_deployment(\n                self.config.k8s_deployment_name,\n                target_instances\n            )\n            if success:\n                success = await self.scaler.wait_for_scaling(\n                    self.config.k8s_deployment_name,\n                    target_instances\n                )\n        else:\n            success = await self.scaler.scale_containers(target_instances)\n        \n        scaling_duration = time.time() - start_time\n        scaling_event['duration_seconds'] = scaling_duration\n        scaling_event['success'] = success\n        \n        if success:\n            self.current_instances = target_instances\n            logger.info(f\"Successfully scaled to {target_instances} instances in {scaling_duration:.1f}s\")\n        else:\n            logger.error(f\"Failed to scale to {target_instances} instances\")\n        \n        if self.test_results:\n            self.test_results.scaling_events.append(scaling_event)\n        \n        return success\n    \n    async def _monitoring_loop(self):\n        \"\"\"Continuous monitoring loop during test.\"\"\"\n        monitor_interval = 10  # Monitor every 10 seconds\n        \n        while self.test_active:\n            try:\n                timestamp = datetime.now()\n                \n                # Get load generator metrics\n                load_metrics = self.load_generator.get_metrics()\n                \n                # Get system metrics (simulated for demo)\n                cpu_usage = min(90, 20 + (load_metrics['current_rps'] / 10))\n                memory_usage = min(80, 30 + (self.current_instances * 5))\n                \n                # Record metrics\n                if self.test_results:\n                    self.test_results.instance_count_history.append((timestamp, self.current_instances))\n                    self.test_results.response_time_history.append((timestamp, load_metrics['avg_response_time_ms']))\n                    self.test_results.throughput_history.append((timestamp, load_metrics['current_rps']))\n                    self.test_results.error_rate_history.append((timestamp, load_metrics['error_rate_percent']))\n                    self.test_results.cpu_usage_history.append((timestamp, cpu_usage))\n                    self.test_results.memory_usage_history.append((timestamp, memory_usage))\n                \n                await asyncio.sleep(monitor_interval)\n                \n            except Exception as e:\n                logger.error(f\"Error in monitoring loop: {e}\")\n                await asyncio.sleep(monitor_interval)\n    \n    async def run_auto_scaling_validation_test(self) -> ScalabilityTestResult:\n        \"\"\"Test auto-scaling behavior with sudden load spikes.\"\"\"\n        logger.info(\"Starting auto-scaling validation test\")\n        \n        start_time = datetime.now()\n        \n        # Initialize test results\n        self.test_results = ScalabilityTestResult(\n            test_name=\"auto_scaling_validation_test\",\n            config=self.config,\n            start_time=start_time,\n            end_time=start_time,\n            instance_count_history=[],\n            scaling_events=[],\n            response_time_history=[],\n            throughput_history=[],\n            error_rate_history=[],\n            cpu_usage_history=[],\n            memory_usage_history=[]\n        )\n        \n        try:\n            # Start with minimum instances\n            await self._scale_to_instances(self.config.min_instances, \"test_setup\")\n            await self.load_generator.start_session()\n            \n            # Start monitoring\n            monitor_task = asyncio.create_task(self._monitoring_loop())\n            \n            # Test scenario: Sudden load spike\n            logger.info(\"Testing sudden load spike...\")\n            \n            # Low load phase\n            await self.load_generator.generate_load(\n                target_rps=self.config.base_load_rps,\n                duration_seconds=120,  # 2 minutes\n                endpoints=self.config.target_endpoints\n            )\n            \n            # Sudden spike\n            spike_task = asyncio.create_task(\n                self.load_generator.generate_load(\n                    target_rps=self.config.max_load_rps,\n                    duration_seconds=300,  # 5 minutes\n                    endpoints=self.config.target_endpoints\n                )\n            )\n            \n            # Monitor and scale during spike\n            scale_task = asyncio.create_task(\n                self._check_and_scale_during_load(300)\n            )\n            \n            await asyncio.gather(spike_task, scale_task)\n            \n            # Cool down\n            await self.load_generator.generate_load(\n                target_rps=self.config.base_load_rps,\n                duration_seconds=180,  # 3 minutes\n                endpoints=self.config.target_endpoints\n            )\n            \n        finally:\n            self.test_active = False\n            monitor_task.cancel()\n            \n            try:\n                await monitor_task\n            except asyncio.CancelledError:\n                pass\n            \n            await self.load_generator.stop_session()\n            \n            if not self.use_kubernetes and hasattr(self.scaler, 'cleanup_containers'):\n                await self.scaler.cleanup_containers()\n        \n        # Finalize results\n        self.test_results.end_time = datetime.now()\n        self.test_results.calculate_summary()\n        \n        logger.info(\n            f\"Auto-scaling validation test completed. \"\n            f\"Effectiveness score: {self.test_results.scaling_effectiveness_score:.1f}\"\n        )\n        \n        return self.test_results\n    \n    def generate_test_report(self, results: List[ScalabilityTestResult], \n                           output_file: str) -> Dict[str, Any]:\n        \"\"\"Generate comprehensive scalability test report.\"\"\"\n        report = {\n            'report_timestamp': datetime.now().isoformat(),\n            'test_configuration': self.config.to_dict(),\n            'test_results': [result.to_dict() for result in results],\n            'summary': self._generate_test_summary(results),\n            'recommendations': self._generate_recommendations(results)\n        }\n        \n        # Save report\n        with open(output_file, 'w') as f:\n            json.dump(report, f, indent=2, default=str)\n        \n        logger.info(f\"Scalability test report saved to {output_file}\")\n        return report\n    \n    def _generate_test_summary(self, results: List[ScalabilityTestResult]) -> Dict[str, Any]:\n        \"\"\"Generate summary statistics across all test results.\"\"\"\n        if not results:\n            return {'error': 'No test results available'}\n        \n        total_tests = len(results)\n        successful_tests = len([r for r in results if r.scaling_effectiveness_score >= 70])\n        \n        avg_effectiveness = np.mean([r.scaling_effectiveness_score for r in results])\n        avg_response_time = np.mean([r.avg_response_time_ms for r in results])\n        avg_error_rate = np.mean([r.avg_error_rate_percent for r in results])\n        total_scaling_events = sum([r.total_scaling_events for r in results])\n        \n        return {\n            'total_tests': total_tests,\n            'successful_tests': successful_tests,\n            'success_rate': successful_tests / total_tests,\n            'avg_effectiveness_score': avg_effectiveness,\n            'avg_response_time_ms': avg_response_time,\n            'avg_error_rate_percent': avg_error_rate,\n            'total_scaling_events': total_scaling_events,\n            'avg_scaling_events_per_test': total_scaling_events / total_tests\n        }\n    \n    def _generate_recommendations(self, results: List[ScalabilityTestResult]) -> List[str]:\n        \"\"\"Generate recommendations based on test results.\"\"\"\n        recommendations = []\n        \n        if not results:\n            return recommendations\n        \n        avg_effectiveness = np.mean([r.scaling_effectiveness_score for r in results])\n        avg_response_time = np.mean([r.avg_response_time_ms for r in results])\n        avg_error_rate = np.mean([r.avg_error_rate_percent for r in results])\n        \n        # Performance recommendations\n        if avg_effectiveness < 70:\n            recommendations.append(\n                f\"Low overall effectiveness score ({avg_effectiveness:.1f}). \"\n                \"Review scaling thresholds and response time targets.\"\n            )\n        \n        if avg_response_time > self.config.max_response_time_ms:\n            recommendations.append(\n                f\"Average response time ({avg_response_time:.1f}ms) exceeds target. \"\n                \"Consider more aggressive scaling or optimizing application performance.\"\n            )\n        \n        if avg_error_rate > self.config.max_error_rate_percent:\n            recommendations.append(\n                f\"Average error rate ({avg_error_rate:.2f}%) exceeds target. \"\n                \"Investigate error causes and improve error handling.\"\n            )\n        \n        # Scaling recommendations\n        excessive_scaling = any(r.total_scaling_events > 10 for r in results)\n        if excessive_scaling:\n            recommendations.append(\n                \"Excessive scaling events detected. \"\n                \"Consider adjusting scaling thresholds to reduce thrashing.\"\n            )\n        \n        minimal_scaling = any(r.total_scaling_events < 2 for r in results)\n        if minimal_scaling:\n            recommendations.append(\n                \"Minimal scaling activity detected. \"\n                \"Verify auto-scaling configuration and thresholds.\"\n            )\n        \n        # General recommendations\n        recommendations.extend([\n            \"Implement proper health checks for faster scaling decisions\",\n            \"Consider implementing predictive scaling based on traffic patterns\",\n            \"Monitor resource utilization trends to optimize instance sizing\",\n            \"Set up proper alerting for scaling events and performance degradation\"\n        ])\n        \n        return recommendations\n\n\nasync def run_comprehensive_scalability_tests():\n    \"\"\"Run comprehensive scalability test suite.\"\"\"\n    # Test configuration\n    config = ScalabilityTestConfig(\n        min_instances=1,\n        max_instances=5,\n        scale_step_size=1,\n        base_load_rps=10.0,\n        max_load_rps=100.0,\n        test_duration_seconds=600,  # 10 minutes for demo\n        target_endpoints=[\"/health\", \"/api/v1/search\"],\n        max_response_time_ms=1000.0\n    )\n    \n    # Create test runner (use Docker for local testing)\n    runner = ScalabilityTestRunner(config, use_kubernetes=False)\n    \n    results = []\n    \n    try:\n        # Test 1: Horizontal scaling test\n        logger.info(\"Running horizontal scaling test...\")\n        horizontal_result = await runner.run_horizontal_scaling_test()\n        results.append(horizontal_result)\n        \n        # Test 2: Auto-scaling validation test\n        logger.info(\"Running auto-scaling validation test...\")\n        autoscaling_result = await runner.run_auto_scaling_validation_test()\n        results.append(autoscaling_result)\n        \n        # Generate comprehensive report\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        report_file = f\"scalability_test_report_{timestamp}.json\"\n        \n        report = runner.generate_test_report(results, report_file)\n        \n        logger.info(\"Scalability testing completed successfully\")\n        return report, results\n        \n    except Exception as e:\n        logger.error(f\"Scalability testing failed: {e}\")\n        raise\n\n\nif __name__ == \"__main__\":\n    import sys\n    \n    # Configure logging\n    logging.basicConfig(\n        level=logging.INFO,\n        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n    )\n    \n    if len(sys.argv) > 1 and sys.argv[1] == \"demo\":\n        # Run a quick demo with mock scaling\n        async def demo():\n            config = ScalabilityTestConfig(\n                min_instances=1,\n                max_instances=3,\n                test_duration_seconds=120,  # 2 minutes\n                base_load_rps=5.0,\n                max_load_rps=20.0\n            )\n            \n            runner = ScalabilityTestRunner(config, use_kubernetes=False)\n            \n            # Mock the scaler for demo\n            class MockScaler:\n                async def scale_containers(self, target_count):\n                    await asyncio.sleep(1)  # Simulate scaling time\n                    return True\n                \n                async def cleanup_containers(self):\n                    pass\n            \n            runner.scaler = MockScaler()\n            \n            try:\n                result = await runner.run_horizontal_scaling_test()\n                print(\"Demo scalability test completed:\")\n                print(json.dumps(result.to_dict(), indent=2, default=str))\n            except Exception as e:\n                print(f\"Demo failed: {e}\")\n        \n        asyncio.run(demo())\n    else:\n        # Run full scalability test suite\n        report, results = asyncio.run(run_comprehensive_scalability_tests())\n        print(\"Scalability testing completed\")\n        print(json.dumps(report, indent=2, default=str))"